{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'autostop.utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-5666bc586de8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_style\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"whitegrid\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mautostop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m marker_dct = {0: u'tickleft', 1: u'tickright', 2: u'tickup', 3: u'tickdown', 4: u'caretleft', u'D': u'diamond', \n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'autostop.utils'"
     ]
    }
   ],
   "source": [
    "#!pip3 install seaborn\n",
    "!export PYTHONPATH=${PYTHONPATH}:/home/guilherme/Downloads/auto-stop-tar/autostop/\n",
    "import re\n",
    "import os\n",
    "import math\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('precision', 3)\n",
    "import subprocess\n",
    "\n",
    "from collections import  defaultdict\n",
    "from operator import itemgetter\n",
    "from scipy import interpolate\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "from autostop.utils import *\n",
    "\n",
    "marker_dct = {0: u'tickleft', 1: u'tickright', 2: u'tickup', 3: u'tickdown', 4: u'caretleft', u'D': u'diamond', \n",
    "              6: u'caretup', 7: u'caretdown', u's': u'square', u'|': u'vline', None: u'nothing', u'None': u'nothing', \n",
    "              u'x': u'x', 5: u'caretright', u'_': u'hline', u'^': u'triangle_up', u' ': u'nothing', \n",
    "              u'd': u'thin_diamond', u'h': u'hexagon1', u'+': u'plus', u'*': u'star', u',': u'pixel', \n",
    "              u'o': u'circle', u'.': u'point', u'1': u'tri_down', u'p': u'pentagon', u'3': u'tri_left', \n",
    "              u'2': u'tri_up', u'4': u'tri_right', u'H': u'hexagon2', u'v': u'triangle_down', u'': u'nothing', \n",
    "              u'8': u'octagon', u'<': u'triangle_left', u'>': u'triangle_right'}\n",
    "\n",
    "show_columns = ['recall', 'cost', 'reliability', 'loss_er', 're']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "tar_master_dir = '/home/guilherme/Downloads/auto-stop-tar'\n",
    "datadir = '/home/guilherme/Downloads/auto-stop-tar/data'\n",
    "retdir = '/home/guilherme/Downloads/auto-stop-tar/ret'\n",
    "# figuredir = '/Users/danli/Documents/Project/autostop/autostop_project/figure'\n",
    "#paperfiguredir = '/Users/danli/Documents/GitProject/autostop/autostop/tois_paper/figures/'#'/Users/danli/Documents/'#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Interaction result and TarRun result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def check_file(filepath):\n",
    "    if not os.path.exists(filepath):\n",
    "        print('not existing file {}'.format(filepath))\n",
    "    if os.path.getsize(filepath) == 0:\n",
    "        print('zero byte file {}'.format(filepath))\n",
    "\n",
    "def get_model_name(model_name):\n",
    "    model = model_name\n",
    "    if 'random' in model_name:\n",
    "        model = '0-random'\n",
    "    if 'knee' in model_name:\n",
    "        model = 'Knee'\n",
    "    if 'scal' in model_name:\n",
    "        model = 'SCAL'\n",
    "    if 'target' in model_name:\n",
    "        model ='Target'\n",
    "    if 'sdtf' in model_name:\n",
    "        model = 'SD-training'\n",
    "    if 'sdfu' in model_name:\n",
    "        model = 'SD-sampling'\n",
    "    if 'autostop' in model_name:\n",
    "        model = 'Ours'\n",
    "    return model\n",
    "\n",
    "\n",
    "def TarEvalResultReader(data_name, model_name, exp_id, train_test, topic_id):\n",
    "    \n",
    "    # run file\n",
    "    runfile = os.path.join(retdir, data_name, 'tar_run', model_name, exp_id, train_test, topic_id + '.run')\n",
    "    check_file(runfile)\n",
    "    \n",
    "    # qrel file\n",
    "    qrelfile = os.path.join(datadir, data_name, 'abs_qrels', topic_id)\n",
    "    check_file(qrelfile)\n",
    "    \n",
    "    # tar eval script\n",
    "    script = os.path.join(tar_master_dir, 'scripts/tar_eval.py')\n",
    "    \n",
    "    # result\n",
    "    ret = subprocess.check_output(['python', script, qrelfile, runfile])\n",
    "    ret = subprocess.check_output([' tail -27 '], shell=True, input=ret)\n",
    "    ret = ret.decode(encoding='utf-8')\n",
    "   \n",
    "    # dataframe\n",
    "    dct = {}\n",
    "    for line in ret.split('\\n'):\n",
    "      if line != '':\n",
    "            tid, key, val = line.split()\n",
    "            if tid == 'ALL':\n",
    "                dct[key] = [float(val)]\n",
    "    \n",
    "    df = pd.DataFrame(dct)\n",
    "    model = get_model_name(model_name)\n",
    "    df['model_name'] = [model]\n",
    "    df['exp_id'] = [exp_id]\n",
    "    df['topic_id'] = [topic_id]\n",
    "    df['recall'] = float(df['rels_found']) / float(df['num_rels'])\n",
    "    df['cost'] = float(df['num_shown']) / float(df['num_docs'])\n",
    "    return df\n",
    "\n",
    "\n",
    "def InteractionLastResult(data_name, model_name, exp_id, train_test, topic_id):\n",
    "    mdir = os.path.join(retdir, data_name, 'interaction', model_name, exp_id, train_test)\n",
    "    filename = topic_id + '.csv'\n",
    "    filepath = os.path.join(mdir, filename)\n",
    "    \n",
    "    check_file(filepath)\n",
    "    model = get_model_name(model_name)\n",
    "    \n",
    "    df = pd.read_csv(filepath)\n",
    "    df = df.iloc[-1:]  # last row\n",
    "    df['model_name'] = [model]\n",
    "    df['exp_id'] = [exp_id]\n",
    "    df['topic_id'] = [topic_id]\n",
    "    return df\n",
    "\n",
    "\n",
    "def InteractionResultReader(data_name, model_name, exp_id, train_test, topic_id):\n",
    "    mdir = os.path.join(retdir, data_name, 'interaction', model_name, exp_id, train_test)\n",
    "    filename = topic_id + '.csv'\n",
    "    filepath = os.path.join(mdir, filename)\n",
    "    \n",
    "    check_file(filepath)\n",
    "    model = get_model_name(model_name)\n",
    "    \n",
    "    df = pd.read_csv(filepath)\n",
    "    \n",
    "    # construct new data\n",
    "    startp, endp, nump = 0.1, 1.0, 10\n",
    "    df = df.drop_duplicates(['sampled_num'])\n",
    "    df['percentage'] = df['sampled_num'] / df['total_num']\n",
    "    max_percentage = df['percentage'].max()\n",
    "    min_percentage = df['percentage'].min()\n",
    "\n",
    "    df['relative_error'] = np.abs(df['total_esti_r'] - df['total_true_r']) / df['total_true_r']\n",
    "    df = df.fillna(0)\n",
    "\n",
    "    x = np.linspace(startp, endp, nump)\n",
    "    f = interpolate.InterpolatedUnivariateSpline(df['percentage'].values, df['relative_error'].values)\n",
    "    relative_error = revised_f(x, f, min_percentage, max_percentage)\n",
    "    f = interpolate.InterpolatedUnivariateSpline(df['percentage'].values, df['running_true_recall'].values)\n",
    "    recall = revised_f(x, f, min_percentage, max_percentage)                                      \n",
    "    f = interpolate.InterpolatedUnivariateSpline(df['percentage'].values, df['ap'].values)\n",
    "    ap = revised_f(x, f, min_percentage, max_percentage)  \n",
    "    \n",
    "    df = pd.DataFrame({\n",
    "                       'model_name': [model] * nump,\n",
    "                        'exp_id': [exp_id] * nump,\n",
    "                       'percentage': x, \n",
    "                       'relative_error': relative_error, \n",
    "                       'recall': recall,\n",
    "                        'ap': ap})\n",
    "    return df\n",
    "\n",
    "\n",
    "def revised_f(x, f, min_percentage, max_percentage):\n",
    "    y = []\n",
    "    miny = f(min_percentage)\n",
    "    maxy = f(max_percentage)\n",
    "    for xi in x:\n",
    "        if xi > max_percentage:\n",
    "            y.append(maxy)\n",
    "        elif xi < min_percentage:\n",
    "            y.append(miny)\n",
    "        else:\n",
    "            y.append(f(xi))\n",
    "    y = np.array(y)\n",
    "    return y\n",
    "\n",
    "\n",
    "def load_data(data_name, folders, exp_ids, topic_ids, func):\n",
    "    dfs = []\n",
    "    for folder in folders:\n",
    "        for exp_id in exp_ids:\n",
    "            for topic_id in topic_ids:\n",
    "                try:\n",
    "                    df = func(data_name, folder, exp_id, 'test', topic_id)\n",
    "                    dfs.append(df)\n",
    "                except Exception as e:\n",
    "                    print(e,folder, exp_id, topic_id)\n",
    "                    pass\n",
    "    df = pd.concat(dfs, axis=0)\n",
    "    return df\n",
    "\n",
    "def pareto_frontier(Xs, Ys, maxX=True, maxY=True):\n",
    "    '''\n",
    "    Method to take two equally-sized lists and return just the elements which lie\n",
    "    on the Pareto frontier, sorted into order.\n",
    "    Default behaviour is to find the maximum for both X and Y, but the option is\n",
    "    available to specify maxX = False or maxY = False to find the minimum for either\n",
    "    or both of the parameters.\n",
    "    '''\n",
    "\n",
    "    # Sort the list in either ascending or descending order of X\n",
    "    myList = sorted([[Xs[i], Ys[i]] for i in range(len(Xs))], reverse=maxX)\n",
    "    # Start the Pareto frontier with the first value in the sorted list\n",
    "    p_front = [myList[0]]\n",
    "    # Loop through the sorted list\n",
    "    for pair in myList[1:]:\n",
    "        if maxY:\n",
    "            if pair[1] >= p_front[-1][1]:  # Look for higher values of Y…\n",
    "                p_front.append(pair)  # … and add them to the Pareto frontier\n",
    "        else:\n",
    "            if pair[1] <= p_front[-1][1]:  # Look for lower values of Y…\n",
    "                p_front.append(pair)  # … and add them to the Pareto frontier\n",
    "    # Turn resulting pairs back into a list of Xs and Ys\n",
    "    p_frontX = [pair[0] for pair in p_front]\n",
    "    p_frontY = [pair[1] for pair in p_front]\n",
    "    return p_frontX, p_frontY\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AutoTAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dfs = []\n",
    "# for topic_id in clef_2017_test_topics:\n",
    "#     _df = TarEvalResultReader(data_name='clef2017', model_name='autotar_sp1.0_sr1.0_cttopicwise_md2_c1.0', exp_id='1', train_test='test', topic_id=topic_id)\n",
    "#     dfs.append(_df)\n",
    "# df = pd.concat(dfs, ignore_index=True)\n",
    "# descdf = df[['wss_100', 'loss_er', 'norm_area', 'recall', 'cost', 'num_shown', 'num_docs', 'rels_found', 'num_rels', 'ap', 'NCG@10', 'NCG@100']]\n",
    "# descdf.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dfs = []\n",
    "# for topic_id in clef_2018_topics:\n",
    "#     _df = TarEvalResultReader(data_name='clef2018', model_name='autotar_sp1.0_sr1.0_cttopicwise_md2_c1.0', exp_id='1', train_test='test', topic_id=topic_id)\n",
    "#     dfs.append(_df)\n",
    "# df = pd.concat(dfs, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dfs = []\n",
    "# for topic_id in clef_2019_topics:\n",
    "#     _df = TarEvalResultReader(data_name='clef2019', model_name='autotar_sp1.0_sr1.0_cttopicwise_md2_c1.0', exp_id='1', train_test='test', topic_id=topic_id)\n",
    "#     dfs.append(_df)\n",
    "# df = pd.concat(dfs, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dfs = []\n",
    "# for topic_id in athome1_topics:\n",
    "#     _df = TarEvalResultReader(data_name='athome1', model_name='autotar_sp0.5_sr1.0_cttopicwise_md2_c1.0', exp_id='1', train_test='test', topic_id=topic_id)\n",
    "#     dfs.append(_df)\n",
    "# df = pd.concat(dfs, ignore_index=True)\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Knee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dfs = []\n",
    "# for topic_id in clef_2017_training_topics:\n",
    "#     for rho in ['dynamic', 0.001,0.01, 0.1, 1, 2, 3, 6, 10]:\n",
    "#         for beta in[100.0, 1000.0]:\n",
    "#             _df = TarEvalResultReader(data_name='clef2017', model_name='knee_sb{}_sp1.0_sr1.0_rho{}_cttopicwise_md2_c1.0'.format(str(beta), str(rho)), exp_id='1', train_test='train', topic_id=topic_id)\n",
    "#             _df['bound'] = rho\n",
    "#             _df['beta'] = beta\n",
    "#             dfs.append(_df)\n",
    "# df = pd.concat(dfs, ignore_index=True)\n",
    "# df = df.groupby(['bound', 'beta']).mean()\n",
    "# df[['cost', 'recall','loss_er']]   # 'wss_100', 'loss_er', 'norm_area', 'recall', 'cost', 'total_cost', 'num_docs','ap'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "for topic_id in clef_2017_test_topics:\n",
    "    for rho in ['dynamic']:\n",
    "        for beta in[1000.0]:\n",
    "            _df = TarEvalResultReader(data_name='clef2017', model_name='knee_sb{}_sp1.0_sr1.0_rho{}_cttopicwise_md2_c1.0'.format(str(beta), str(rho)), exp_id='1', train_test='test', topic_id=topic_id)\n",
    "            _df['bound'] = rho\n",
    "            _df['beta'] = beta\n",
    "            dfs.append(_df)\n",
    "df = pd.concat(dfs, ignore_index=True)\n",
    "df = df.groupby(['bound', 'beta']).mean()\n",
    "\n",
    "df[['cost', 'recall','loss_er']]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "for topic_id in athome1_topics:\n",
    "    for rho in ['dynamic']:\n",
    "        for beta in[1000.0]:\n",
    "            _df = TarEvalResultReader(data_name='athome1', model_name='knee_sb{}_sp1.0_sr1.0_rho{}_cttopicwise_md2_c1.0'.format(str(beta), str(rho)), exp_id='1', train_test='test', topic_id=topic_id)\n",
    "            _df['bound'] = rho\n",
    "            _df['beta'] = beta\n",
    "            dfs.append(_df)\n",
    "df = pd.concat(dfs, ignore_index=True)\n",
    "# df = df.groupby(['bound', 'beta']).mean()\n",
    "\n",
    "df[['cost', 'num_shown',  'recall','loss_er']]  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Knee: $\\rho$, $\\beta$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## athome1 athome4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "for topic_id in athome1_topics:\n",
    "    for rho in ['dynamic', 3, 6, 10]:\n",
    "        for beta in[100.0, 1000.0]:\n",
    "            _df = TarEvalResultReader(data_name='athome1', model_name='knee_sb{}_sp1.0_sr1.0_rho{}_cttopicwise_md2_c1.0'.format(str(beta), str(rho)), exp_id='1', train_test='train', topic_id=topic_id)\n",
    "            _df['rho'] = rho\n",
    "            _df['beta'] = beta\n",
    "            dfs.append(_df)\n",
    "df = pd.concat(dfs, ignore_index=True)\n",
    "df['reliability'] = df.apply(lambda row:1 if row['recall'] >= 1.0 else 0, axis=1)\n",
    "df['re'] = np.abs(df['recall'] - 1.0) / 1.0\n",
    "df = df.groupby(['rho', 'beta']).mean()\n",
    "df[show_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "for topic_id in athome4_topics:\n",
    "    for rho in [10]:\n",
    "        for beta in[1000.0]:\n",
    "            _df = TarEvalResultReader(data_name='athome4', model_name='knee_sb{}_sp1.0_sr1.0_rho{}_cttopicwise_md2_c1.0'.format(str(beta), str(rho)), exp_id='1', train_test='test', topic_id=topic_id)\n",
    "            _df['rho'] = rho\n",
    "            _df['beta'] = beta\n",
    "            dfs.append(_df)\n",
    "df = pd.concat(dfs, ignore_index=True)\n",
    "df['reliability'] = df.apply(lambda row:1 if row['recall'] >= 1.0 else 0, axis=1)\n",
    "df['re'] = np.abs(df['recall'] - 1.0) / 1.0\n",
    "descdf = df[show_columns]\n",
    "descdf.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## legal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "for topic_id in legal_t10_interactive_training_topics:\n",
    "    for rho in ['dynamic', 3, 6, 10]:\n",
    "        for beta in[100.0, 1000.0]:\n",
    "            _df = TarEvalResultReader(data_name='legal', model_name='knee_sb{}_sp1.0_sr1.0_rho{}_cttopicwise_md2_c1.0'.format(str(beta), str(rho)), exp_id='1', train_test='train', topic_id=topic_id)\n",
    "            _df['rho'] = rho\n",
    "            _df['beta'] = beta\n",
    "            dfs.append(_df)\n",
    "df = pd.concat(dfs, ignore_index=True)\n",
    "df['reliability'] = df.apply(lambda row:1 if row['recall'] >= 1.0 else 0, axis=1)\n",
    "df = df.groupby(['rho', 'beta']).mean()\n",
    "df[show_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "for topic_id in legal_t10_interactive_test_topics:\n",
    "    for rho in ['dynamic']:\n",
    "        for beta in[1000.0]:\n",
    "            _df = TarEvalResultReader(data_name='legal', model_name='knee_sb{}_sp1.0_sr1.0_rho{}_cttopicwise_md2_c1.0'.format(str(beta), str(rho)), exp_id='1', train_test='test', topic_id=topic_id)\n",
    "            _df['rho'] = rho\n",
    "            _df['beta'] = beta\n",
    "            dfs.append(_df)\n",
    "df = pd.concat(dfs, ignore_index=True)\n",
    "df['reliability'] = df.apply(lambda row:1 if row['recall'] >= 1.0 else 0, axis=1)\n",
    "df['re'] = np.abs(df['recall'] - 1.0) / 1.0\n",
    "descdf = df[show_columns]\n",
    "descdf.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## clef 2017/2018/2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "for topic_id in clef_2017_training_topics:\n",
    "    for rho in ['dynamic', 3, 6, 10]:\n",
    "        for beta in[100.0, 1000.0]:\n",
    "            _df = TarEvalResultReader(data_name='clef2017', model_name='knee_sb{}_sp1.0_sr1.0_rho{}_cttopicwise_md2_c1.0'.format(str(beta), str(rho)), exp_id='1', train_test='train', topic_id=topic_id)\n",
    "            _df['rho'] = rho\n",
    "            _df['beta'] = beta\n",
    "            dfs.append(_df)\n",
    "df = pd.concat(dfs, ignore_index=True)\n",
    "df['reliability'] = df.apply(lambda row:1 if row['recall'] >= 1.0 else 0, axis=1)\n",
    "df = df.groupby(['rho', 'beta']).mean()\n",
    "df[['wss_100', 'loss_er', 'norm_area', 'recall', 'cost', 'num_shown', 'num_docs', 'rels_found', 'num_rels', 'ap', 'NCG@10', 'NCG@100', 'reliability']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "for topic_id in clef_2017_test_topics:\n",
    "    for rho in ['dynamic']:\n",
    "        for beta in[1000.0]:\n",
    "            _df = TarEvalResultReader(data_name='clef2017', model_name='knee_sb{}_sp1.0_sr1.0_rho{}_cttopicwise_md2_c1.0'.format(str(beta), str(rho)), exp_id='1', train_test='test', topic_id=topic_id)\n",
    "            _df['rho'] = rho\n",
    "            _df['beta'] = beta\n",
    "            dfs.append(_df)\n",
    "df = pd.concat(dfs, ignore_index=True)\n",
    "df['reliability'] = df.apply(lambda row:1 if row['recall'] >= 1.0 else 0, axis=1)\n",
    "df['re'] = np.abs(df['recall'] - 1.0) / 1.0\n",
    "descdf = df[show_columns]\n",
    "descdf.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "for topic_id in clef_2018_topics:\n",
    "    for rho in ['dynamic']:\n",
    "        for beta in[1000.0]:\n",
    "            _df = TarEvalResultReader(data_name='clef2018', model_name='knee_sb{}_sp1.0_sr1.0_rho{}_cttopicwise_md2_c1.0'.format(str(beta), str(rho)), exp_id='1', train_test='test', topic_id=topic_id)\n",
    "            _df['rho'] = rho\n",
    "            _df['beta'] = beta\n",
    "            dfs.append(_df)\n",
    "df = pd.concat(dfs, ignore_index=True)\n",
    "df['reliability'] = df.apply(lambda row:1 if row['recall'] >= 1.0 else 0, axis=1)\n",
    "df['re'] = np.abs(df['recall'] - 1.0) / 1.0\n",
    "descdf = df[show_columns]\n",
    "descdf.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "for topic_id in clef_2019_topics:\n",
    "    for rho in ['dynamic']:\n",
    "        for beta in[1000.0]:\n",
    "            _df = TarEvalResultReader(data_name='clef2019', model_name='knee_sb{}_sp1.0_sr1.0_rho{}_cttopicwise_md2_c1.0'.format(str(beta), str(rho)), exp_id='1', train_test='test', topic_id=topic_id)\n",
    "            _df['rho'] = rho\n",
    "            _df['beta'] = beta\n",
    "            dfs.append(_df)\n",
    "df = pd.concat(dfs, ignore_index=True)\n",
    "df['reliability'] = df.apply(lambda row:1 if row['recall'] >= 1.0 else 0, axis=1)\n",
    "df['re'] = np.abs(df['recall'] - 1.0) / 1.0\n",
    "descdf = df[show_columns]\n",
    "descdf.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SD-training "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## clef 2017 2018 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "for topic_id in clef_2017_test_topics:\n",
    "    for tr in [0.8, 0.9, 0.99]:\n",
    "        _df = TarEvalResultReader(data_name='clef2017', model_name='sdtf_tr{}'.format(str(tr)), exp_id='1', train_test='test', topic_id=topic_id)\n",
    "        _df['tr'] = tr\n",
    "        dfs.append(_df)\n",
    "df = pd.concat(dfs, ignore_index=True)\n",
    "df['reliability'] = df.apply(lambda row:1 if row['recall'] >= row['tr'] else 0, axis=1)\n",
    "df['re'] = np.abs(df['recall'] - df['tr']) / df['tr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descdf = df[df['tr'] == 0.8][show_columns]\n",
    "descdf.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descdf = df[df['tr'] == 0.9][show_columns]\n",
    "descdf.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descdf = df[df['tr'] == 0.99][show_columns]\n",
    "descdf.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### clef 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "for topic_id in clef_2018_topics:\n",
    "    for tr in [0.8, 0.9, 0.99]:\n",
    "        _df = TarEvalResultReader(data_name='clef2018', model_name='sdtf_tr{}'.format(str(tr)), exp_id='1', train_test='test', topic_id=topic_id)\n",
    "        _df['tr'] = tr\n",
    "        dfs.append(_df)\n",
    "df = pd.concat(dfs, ignore_index=True)\n",
    "df['reliability'] = df.apply(lambda row:1 if row['recall'] >= row['tr'] else 0, axis=1)\n",
    "df['re'] = np.abs(df['recall'] - df['tr']) / df['tr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descdf = df[df['tr'] == 0.8][show_columns]\n",
    "descdf.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descdf = df[df['tr'] == 0.9][show_columns]\n",
    "descdf.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descdf = df[df['tr'] == 0.99][show_columns]\n",
    "descdf.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### clef 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "for topic_id in clef_2019_topics:\n",
    "    for tr in [0.8, 0.9, 0.99]:\n",
    "        _df = TarEvalResultReader(data_name='clef2019', model_name='sdtf_tr{}'.format(str(tr)), exp_id='1', train_test='test', topic_id=topic_id)\n",
    "        _df['tr'] = tr\n",
    "        dfs.append(_df)\n",
    "df = pd.concat(dfs, ignore_index=True)\n",
    "df['reliability'] = df.apply(lambda row:1 if row['recall'] >= row['tr'] else 0, axis=1)\n",
    "df['re'] = np.abs(df['recall'] - df['tr']) / df['tr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descdf = df[df['tr'] == 0.8][show_columns]\n",
    "descdf.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descdf = df[df['tr'] == 0.9][show_columns]\n",
    "descdf.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descdf = df[df['tr'] == 0.99][show_columns]\n",
    "descdf.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### athome4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "for topic_id in athome4_topics:\n",
    "    for tr in [0.8, 0.9, 0.99]:\n",
    "        _df = TarEvalResultReader(data_name='athome4', model_name='sdtf_tr{}'.format(str(tr)), exp_id='1', train_test='test', topic_id=topic_id)\n",
    "        _df['tr'] = tr\n",
    "        dfs.append(_df)\n",
    "df = pd.concat(dfs, ignore_index=True)\n",
    "df['reliability'] = df.apply(lambda row:1 if row['recall'] >= row['tr'] else 0, axis=1)\n",
    "df['re'] = np.abs(df['recall'] - df['tr']) / df['tr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descdf = df[df['tr'] == 0.8][show_columns]\n",
    "descdf.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descdf = df[df['tr'] == 0.9][show_columns]\n",
    "descdf.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descdf = df[df['tr'] == 0.99][show_columns]\n",
    "descdf.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## legal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "for topic_id in legal_t10_interactive_test_topics:\n",
    "    for target_recall in [0.8, 0.9, 0.99]:\n",
    "        try:\n",
    "            _df = TarEvalResultReader(data_name='legal', model_name='sdtf_tr{}'.format(str(target_recall)), exp_id='1', train_test='test', topic_id=topic_id)\n",
    "            _df['tr'] = target_recall\n",
    "            dfs.append(_df)\n",
    "        except:\n",
    "            print(topic_id)\n",
    "df = pd.concat(dfs, ignore_index=True)\n",
    "df['reliability'] = df.apply(lambda row:1 if row['recall'] > row['tr'] else 0, axis=1)\n",
    "df['re'] = np.abs(df['recall'] - df['tr']) / df['tr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descdf = df[df['tr'] == 0.8][show_columns]\n",
    "descdf.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descdf = df[df['tr'] == 0.9][show_columns]\n",
    "descdf.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descdf = df[df['tr'] == 0.99][show_columns]\n",
    "descdf.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SD-sampling\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "for topic_id in athome1_topics:\n",
    "    for target_recall in [0.8, 0.9, 0.99]:\n",
    "        for sample_percentage in [0.1, 0.2, 0.3, 0.4, 0.5]:\n",
    "            _df = TarEvalResultReader(data_name='athome1', model_name='sdfu_smp{}_tr{}'.format(str(sample_percentage),str(target_recall)), exp_id='1', train_test='train', topic_id=topic_id)\n",
    "            _df['tr'] = target_recall\n",
    "            _df['smp'] = sample_percentage\n",
    "            dfs.append(_df)\n",
    "df = pd.concat(dfs, ignore_index=True)\n",
    "df['reliability'] = df.apply(lambda row:1 if row['recall'] > row['tr'] else 0, axis=1)\n",
    "df = df.groupby(['tr', 'smp']).mean()\n",
    "df[['wss_100', 'loss_er', 'norm_area', 'recall', 'cost', 'num_shown', 'num_docs', 'rels_found', 'num_rels', 'ap', 'NCG@10', 'NCG@100', 'reliability']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## athome 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "for topic_id in athome4_topics:\n",
    "    for target_recall in [0.8, 0.9, 0.99]:\n",
    "        for sample_percentage in [0.1]:\n",
    "            _df = TarEvalResultReader(data_name='athome4', model_name='sdfu_smp{}_tr{}'.format(str(sample_percentage),str(target_recall)), exp_id='1', train_test='test', topic_id=topic_id)\n",
    "            _df['tr'] = target_recall\n",
    "            _df['smp'] = sample_percentage\n",
    "            dfs.append(_df)\n",
    "df = pd.concat(dfs, ignore_index=True)\n",
    "df['reliability'] = df.apply(lambda row:1 if row['recall'] > row['tr'] else 0, axis=1)\n",
    "df['re'] = np.abs(df['recall'] - df['tr']) / df['tr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descdf = df[df['tr'] == 0.8][show_columns]\n",
    "descdf.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descdf = df[df['tr'] == 0.9][show_columns]\n",
    "descdf.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descdf = df[df['tr'] == 0.99][show_columns]\n",
    "descdf.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## clef 2017 2018 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "for topic_id in clef_2017_training_topics:\n",
    "    for target_recall in [0.8, 0.9, 0.99]:\n",
    "        for sample_percentage in [0.1, 0.2, 0.3, 0.4, 0.5]:\n",
    "            _df = TarEvalResultReader(data_name='clef2017', model_name='sdfu_smp{}_tr{}'.format(str(sample_percentage),str(target_recall)), exp_id='1', train_test='train', topic_id=topic_id)\n",
    "            _df['tr'] = target_recall\n",
    "            _df['smp'] = sample_percentage\n",
    "            dfs.append(_df)\n",
    "df = pd.concat(dfs, ignore_index=True)\n",
    "df['reliability'] = df.apply(lambda row:1 if row['recall'] > row['tr'] else 0, axis=1)\n",
    "df = df.groupby(['tr', 'smp']).mean()\n",
    "df[['wss_100', 'loss_er', 'norm_area', 'recall', 'cost', 'num_shown', 'num_docs', 'rels_found', 'num_rels', 'ap', 'NCG@10', 'NCG@100', 'reliability']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "for topic_id in clef_2017_test_topics:\n",
    "    for (target_recall,sample_percentage) in [(0.80,0.4), (0.90, 0.3), (0.99, 0.2)]:\n",
    "        _df = TarEvalResultReader(data_name='clef2017', model_name='sdfu_smp{}_tr{}'.format(str(sample_percentage),str(target_recall)), exp_id='1', train_test='test', topic_id=topic_id)\n",
    "        _df['tr'] = target_recall\n",
    "        _df['smp'] = sample_percentage\n",
    "        dfs.append(_df)\n",
    "df = pd.concat(dfs, ignore_index=True)\n",
    "df['reliability'] = df.apply(lambda row:1 if row['recall'] > row['tr'] else 0, axis=1)\n",
    "df['re'] = np.abs(df['recall'] - df['tr']) / df['tr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descdf = df[df['tr'] == 0.8][show_columns]\n",
    "descdf.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descdf = df[df['tr'] == 0.9][show_columns]\n",
    "descdf.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descdf = df[df['tr'] == 0.99][show_columns]\n",
    "descdf.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "for topic_id in clef_2018_topics:\n",
    "    for (target_recall,sample_percentage) in [(0.80,0.4), (0.90, 0.3), (0.99, 0.2)]:\n",
    "        _df = TarEvalResultReader(data_name='clef2018', model_name='sdfu_smp{}_tr{}'.format(str(sample_percentage),str(target_recall)), exp_id='1', train_test='test', topic_id=topic_id)\n",
    "        _df['tr'] = target_recall\n",
    "        _df['smp'] = sample_percentage\n",
    "        dfs.append(_df)\n",
    "df = pd.concat(dfs, ignore_index=True)\n",
    "df['reliability'] = df.apply(lambda row:1 if row['recall'] > row['tr'] else 0, axis=1)\n",
    "df['re'] = np.abs(df['recall'] - df['tr']) / df['tr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descdf = df[df['tr'] == 0.8][show_columns]\n",
    "descdf.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descdf = df[df['tr'] == 0.9][show_columns]\n",
    "descdf.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descdf = df[df['tr'] == 0.99][show_columns]\n",
    "descdf.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for topic_id in clef_2019_topics:\n",
    "    for (target_recall,sample_percentage) in [(0.80,0.4), (0.90, 0.3), (0.99, 0.2)]:\n",
    "        _df = TarEvalResultReader(data_name='clef2019', model_name='sdfu_smp{}_tr{}'.format(str(sample_percentage),str(target_recall)), exp_id='1', train_test='test', topic_id=topic_id)\n",
    "        _df['tr'] = target_recall\n",
    "        _df['smp'] = sample_percentage\n",
    "        dfs.append(_df)\n",
    "df = pd.concat(dfs, ignore_index=True)\n",
    "df['reliability'] = df.apply(lambda row:1 if row['recall'] > row['tr'] else 0, axis=1)\n",
    "df['re'] = np.abs(df['recall'] - df['tr']) / df['tr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descdf = df[df['tr'] == 0.8][show_columns]\n",
    "descdf.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descdf = df[df['tr'] == 0.9][show_columns]\n",
    "descdf.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descdf = df[df['tr'] == 0.99][show_columns]\n",
    "descdf.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## legal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "for topic_id in legal_t10_interactive_training_topics:\n",
    "    for target_recall in [0.8, 0.9, 0.99]:\n",
    "        for sample_percentage in [0.1, 0.2, 0.3, 0.4, 0.5]:\n",
    "            _df = TarEvalResultReader(data_name='legal', model_name='sdfu_smp{}_tr{}'.format(sample_percentage, target_recall), exp_id='1', train_test='train', topic_id=topic_id)\n",
    "            _df['tr'] = target_recall\n",
    "            _df['smp'] = sample_percentage\n",
    "            dfs.append(_df)\n",
    "df = pd.concat(dfs, ignore_index=True)\n",
    "df['reliability'] = df.apply(lambda row:1 if row['recall'] >= row['tr'] else 0, axis=1)\n",
    "df = df.groupby(['tr', 'smp']).mean()\n",
    "df[['wss_100', 'loss_er', 'norm_area', 'recall', 'cost', 'num_shown', 'num_docs', 'rels_found', 'num_rels', 'ap', 'NCG@10', 'NCG@100', 'reliability']]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "for topic_id in legal_t10_interactive_test_topics:\n",
    "    for target_recall in [0.8, 0.9, 0.99]:\n",
    "        for sample_percentage in [0.1]:\n",
    "            _df = TarEvalResultReader(data_name='legal', model_name='sdfu_smp{}_tr{}'.format(sample_percentage, target_recall), exp_id='1', train_test='test', topic_id=topic_id)\n",
    "            _df['tr'] = target_recall\n",
    "            _df['smp'] = sample_percentage\n",
    "            dfs.append(_df)\n",
    "df = pd.concat(dfs, ignore_index=True)\n",
    "df['reliability'] = df.apply(lambda row:1 if row['recall'] >= row['tr'] else 0, axis=1)\n",
    "df['re'] = np.abs(df['recall'] - df['tr']) / df['tr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descdf = df[df['tr'] == 0.8][show_columns]\n",
    "descdf.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descdf = df[df['tr'] == 0.9][show_columns]\n",
    "descdf.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descdf = df[df['tr'] == 0.99][show_columns]\n",
    "descdf.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target: target_rel_num"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## clef 2017 2018 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "for topic_id in clef_2017_training_topics:\n",
    "    for target_rel_num in [5, 10, 15]:\n",
    "        _df = TarEvalResultReader(data_name='clef2017', model_name='target_sp1.0_sr1.0_md2_c1.0_trn{}'.format(target_rel_num), exp_id='1', train_test='train', topic_id=topic_id)\n",
    "        _df['trn'] = target_rel_num\n",
    "        dfs.append(_df)\n",
    "df = pd.concat(dfs, ignore_index=True)\n",
    "df['reliability'] = df.apply(lambda row:1 if row['recall'] >= 1.0 else 0, axis=1)\n",
    "df = df.groupby(['trn']).mean()\n",
    "df[['wss_100', 'loss_er', 'norm_area', 'recall', 'cost', 'num_shown', 'num_docs', 'rels_found', 'num_rels', 'ap', 'NCG@10', 'NCG@100', 'reliability']]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "for topic_id in clef_2017_test_topics:\n",
    "    target_rel_num =15\n",
    "    _df = TarEvalResultReader(data_name='clef2017', model_name='target_sp1.0_sr1.0_md2_c1.0_trn{}'.format(target_rel_num), exp_id='1', train_test='train', topic_id=topic_id)\n",
    "    dfs.append(_df)\n",
    "df = pd.concat(dfs, ignore_index=True)\n",
    "df['reliability'] = df.apply(lambda row:1 if row['recall'] >= 1.0 else 0, axis=1)\n",
    "df['re'] = np.abs(df['recall'] - 1.0) / 1.0\n",
    "descdf = df[show_columns]\n",
    "descdf.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "for topic_id in clef_2018_topics:\n",
    "    target_rel_num =15\n",
    "    _df = TarEvalResultReader(data_name='clef2018', model_name='target_sp1.0_sr1.0_md2_c1.0_trn{}'.format(target_rel_num), exp_id='1', train_test='train', topic_id=topic_id)\n",
    "    dfs.append(_df)\n",
    "df = pd.concat(dfs, ignore_index=True)\n",
    "df['reliability'] = df.apply(lambda row:1 if row['recall'] >= 1.0 else 0, axis=1)\n",
    "df['re'] = np.abs(df['recall'] - 1.0) / 1.0\n",
    "descdf = df[show_columns]\n",
    "descdf.describe()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "for topic_id in clef_2019_topics:\n",
    "    target_rel_num =15\n",
    "    _df = TarEvalResultReader(data_name='clef2019', model_name='target_sp1.0_sr1.0_md2_c1.0_trn{}'.format(target_rel_num), exp_id='1', train_test='train', topic_id=topic_id)\n",
    "    dfs.append(_df)\n",
    "df = pd.concat(dfs, ignore_index=True)\n",
    "df['reliability'] = df.apply(lambda row:1 if row['recall'] >= 1.0 else 0, axis=1)\n",
    "df['re'] = np.abs(df['recall'] - 1.0) / 1.0\n",
    "descdf = df[show_columns]\n",
    "descdf.describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## athome1 athome4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "for topic_id in athome1_topics:\n",
    "    for target_rel_num in [5, 10, 15]:\n",
    "        _df = TarEvalResultReader(data_name='athome1', model_name='target_sp1.0_sr1.0_md2_c1.0_trn{}'.format(target_rel_num), exp_id='1', train_test='train', topic_id=topic_id)\n",
    "        _df['trn'] = target_rel_num\n",
    "        dfs.append(_df)\n",
    "df = pd.concat(dfs, ignore_index=True)\n",
    "df['reliability'] = df.apply(lambda row:1 if row['recall'] >= 1.0 else 0, axis=1)\n",
    "df = df.groupby(['trn']).mean()\n",
    "df[['wss_100', 'loss_er', 'norm_area', 'recall', 'cost', 'num_shown', 'num_docs', 'rels_found', 'num_rels', 'ap', 'NCG@10', 'NCG@100', 'reliability']]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "for topic_id in athome4_topics:\n",
    "    target_rel_num =15\n",
    "    try:\n",
    "        _df = TarEvalResultReader(data_name='athome4', model_name='target_sp1.0_sr1.0_md2_c1.0_trn{}'.format(target_rel_num), exp_id='1', train_test='train', topic_id=topic_id)\n",
    "        dfs.append(_df)\n",
    "    except:\n",
    "        print(topic_id)\n",
    "df = pd.concat(dfs, ignore_index=True)\n",
    "df['reliability'] = df.apply(lambda row:1 if row['recall'] >= 1.0 else 0, axis=1)\n",
    "df['re'] = np.abs(df['recall'] - 1.0) / 1.0\n",
    "descdf = df[show_columns]\n",
    "descdf.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## legal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "for topic_id in legal_t10_interactive_training_topics:\n",
    "    for target_rel_num in [5, 10, 15]:\n",
    "        _df = TarEvalResultReader(data_name='legal', model_name='target_sp1.0_sr1.0_md2_c1.0_trn{}'.format(target_rel_num), exp_id='1', train_test='train', topic_id=topic_id)\n",
    "        _df['trn'] = target_rel_num\n",
    "        dfs.append(_df)\n",
    "df = pd.concat(dfs, ignore_index=True)\n",
    "df['reliability'] = df.apply(lambda row:1 if row['recall'] >= 1.0 else 0, axis=1)\n",
    "df = df.groupby(['trn']).mean()\n",
    "df[['wss_100', 'loss_er', 'norm_area', 'recall', 'cost', 'num_shown', 'num_docs', 'rels_found', 'num_rels', 'ap', 'NCG@10', 'NCG@100', 'reliability']]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "for topic_id in legal_t10_interactive_test_topics:\n",
    "    for target_rel_num in [10]:\n",
    "        _df = TarEvalResultReader(data_name='legal', model_name='target_sp1.0_sr1.0_md2_c1.0_trn{}'.format(target_rel_num), exp_id='1', train_test='train', topic_id=topic_id)\n",
    "        _df['trn'] = target_rel_num\n",
    "        dfs.append(_df)\n",
    "df = pd.concat(dfs, ignore_index=True)\n",
    "df['reliability'] = df.apply(lambda row:1 if row['recall'] >= 1.0 else 0, axis=1)\n",
    "df['re'] = np.abs(df['recall'] - 1.0) / 1.0\n",
    "descdf = df[show_columns]\n",
    "descdf.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SCAL: bound_bt, sub_percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "for topic_id in athome1_topics:\n",
    "    for sub_percentage in [0.8, 1.0]:\n",
    "        for bound_bt in [30, 50, 70, 90, 110]:\n",
    "            for ita in [1.0, 1.05]:\n",
    "                for tr in [0.8, 0.9, 1.0]:\n",
    "                    _df = TarEvalResultReader(data_name='athome1', model_name='scal_sp1.0_sr1.0_tr{}_md2_c1.0_spt{}_bnd{}_min_bktsamplerel_ita{}'.format(tr, sub_percentage, bound_bt, ita), exp_id='1', train_test='train', topic_id=topic_id)\n",
    "                    _df['tr'] = tr\n",
    "                    _df['spt'] = sub_percentage\n",
    "                    _df['bnd'] = bound_bt\n",
    "                    _df['ita'] = ita\n",
    "                    dfs.append(_df)\n",
    "df = pd.concat(dfs, ignore_index=True)\n",
    "df['reliability'] = df.apply(lambda row:1 if row['recall'] >= row['tr'] else 0, axis=1)\n",
    "df = df.groupby(['tr', 'spt', 'bnd', 'ita']).mean()\n",
    "df[['wss_100', 'loss_er', 'norm_area', 'recall', 'cost', 'num_shown', 'num_docs', 'rels_found', 'num_rels', 'ap', 'NCG@10', 'NCG@100', 'reliability']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "for topic_id in athome4_topics:\n",
    "    sub_percentage = 0.8\n",
    "    bound_bt = 70\n",
    "    ita = 1.05\n",
    "    tr = 0.8\n",
    "    _df = TarEvalResultReader(data_name='athome4', model_name='scal_sp1.0_sr1.0_tr{}_md2_c1.0_spt{}_bnd{}_min_bktsamplerel_ita{}'.format(tr, sub_percentage, bound_bt, ita), exp_id='1', train_test='test', topic_id=topic_id)\n",
    "    _df['tr'] = tr\n",
    "    _df['spt'] = sub_percentage\n",
    "    _df['bnd'] = bound_bt\n",
    "    _df['ita'] = ita\n",
    "    dfs.append(_df)\n",
    "df = pd.concat(dfs, ignore_index=True)\n",
    "df['reliability'] = df.apply(lambda row:1 if row['recall'] >= row['tr'] else 0, axis=1)\n",
    "df['re'] = np.abs(df['recall'] - df['tr']) / df['tr']\n",
    "descdf = df[show_columns]\n",
    "descdf.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "for topic_id in athome4_topics:\n",
    "    sub_percentage = 1.0\n",
    "    bound_bt = 110\n",
    "    ita = 1.05\n",
    "    tr = 0.9\n",
    "    _df = TarEvalResultReader(data_name='athome4', model_name='scal_sp1.0_sr1.0_tr{}_md2_c1.0_spt{}_bnd{}_min_bktsamplerel_ita{}'.format(tr, sub_percentage, bound_bt, ita), exp_id='1', train_test='test', topic_id=topic_id)\n",
    "    _df['tr'] = tr\n",
    "    _df['spt'] = sub_percentage\n",
    "    _df['bnd'] = bound_bt\n",
    "    _df['ita'] = ita\n",
    "    dfs.append(_df)\n",
    "df = pd.concat(dfs, ignore_index=True)\n",
    "df['reliability'] = df.apply(lambda row:1 if row['recall'] >= row['tr'] else 0, axis=1)\n",
    "df['re'] = np.abs(df['recall'] - df['tr']) / df['tr']\n",
    "descdf = df[show_columns]\n",
    "descdf.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "for topic_id in athome4_topics:\n",
    "    sub_percentage = 1.0\n",
    "    bound_bt = 110\n",
    "    ita = 1.05\n",
    "    tr = 1.0\n",
    "    _df = TarEvalResultReader(data_name='athome4', model_name='scal_sp1.0_sr1.0_tr{}_md2_c1.0_spt{}_bnd{}_min_bktsamplerel_ita{}'.format(tr, sub_percentage, bound_bt, ita), exp_id='1', train_test='test', topic_id=topic_id)\n",
    "    _df['tr'] = tr\n",
    "    _df['spt'] = sub_percentage\n",
    "    _df['bnd'] = bound_bt\n",
    "    _df['ita'] = ita\n",
    "    dfs.append(_df)\n",
    "df = pd.concat(dfs, ignore_index=True)\n",
    "df['reliability'] = df.apply(lambda row:1 if row['recall'] >= row['tr'] else 0, axis=1)\n",
    "df['re'] = np.abs(df['recall'] - df['tr']) / df['tr']\n",
    "descdf = df[show_columns]\n",
    "descdf.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## legal "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "for topic_id in legal_t10_interactive_training_topics:\n",
    "    for sub_percentage in [0.8, 1.0]:\n",
    "        for bound_bt in [30, 50, 70, 90, 110]:\n",
    "            for ita in [1.0, 1.05]:\n",
    "                for tr in [0.8, 0.9, 1.0]:\n",
    "                    _df = TarEvalResultReader(data_name='legal', model_name='scal_sp1.0_sr1.0_tr{}_md2_c1.0_spt{}_bnd{}_min_bktsamplerel_ita{}'.format(tr, sub_percentage, bound_bt, ita), exp_id='1', train_test='train', topic_id=topic_id)\n",
    "                    _df['tr'] = tr\n",
    "                    _df['spt'] = sub_percentage\n",
    "                    _df['bnd'] = bound_bt\n",
    "                    _df['ita'] = ita\n",
    "                    dfs.append(_df)\n",
    "df = pd.concat(dfs, ignore_index=True)\n",
    "df['reliability'] = df.apply(lambda row:1 if row['recall'] >= row['tr'] else 0, axis=1)\n",
    "df = df.groupby(['tr', 'spt', 'bnd', 'ita']).mean()\n",
    "df[['wss_100', 'loss_er', 'norm_area', 'recall', 'cost', 'num_shown', 'num_docs', 'rels_found', 'num_rels', 'ap', 'NCG@10', 'NCG@100', 'reliability']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "for topic_id in legal_t10_interactive_test_topics:\n",
    "    sub_percentage = 1.0\n",
    "    bound_bt = 30\n",
    "    ita = 1.05\n",
    "    tr = 0.8\n",
    "    _df = TarEvalResultReader(data_name='legal', model_name='scal_sp1.0_sr1.0_tr{}_md2_c1.0_spt{}_bnd{}_min_bktsamplerel_ita{}'.format(tr, sub_percentage, bound_bt, ita), exp_id='1', train_test='test', topic_id=topic_id)\n",
    "    _df['tr'] = tr\n",
    "    _df['spt'] = sub_percentage\n",
    "    _df['bnd'] = bound_bt\n",
    "    _df['ita'] = ita\n",
    "    dfs.append(_df)\n",
    "df = pd.concat(dfs, ignore_index=True)\n",
    "df['reliability'] = df.apply(lambda row:1 if row['recall'] >= row['tr'] else 0, axis=1)\n",
    "df['re'] = np.abs(df['recall'] - df['tr']) / df['tr']\n",
    "descdf = df[show_columns]\n",
    "descdf.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "for topic_id in legal_t10_interactive_test_topics:\n",
    "    sub_percentage = 1.0\n",
    "    bound_bt = 30\n",
    "    ita = 1.05\n",
    "    tr = 0.9\n",
    "    _df = TarEvalResultReader(data_name='legal', model_name='scal_sp1.0_sr1.0_tr{}_md2_c1.0_spt{}_bnd{}_min_bktsamplerel_ita{}'.format(tr, sub_percentage, bound_bt, ita), exp_id='1', train_test='test', topic_id=topic_id)\n",
    "    _df['tr'] = tr\n",
    "    _df['spt'] = sub_percentage\n",
    "    _df['bnd'] = bound_bt\n",
    "    _df['ita'] = ita\n",
    "    dfs.append(_df)\n",
    "df = pd.concat(dfs, ignore_index=True)\n",
    "df['reliability'] = df.apply(lambda row:1 if row['recall'] >= row['tr'] else 0, axis=1)\n",
    "df['re'] = np.abs(df['recall'] - df['tr']) / df['tr']\n",
    "descdf = df[show_columns]\n",
    "descdf.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "for topic_id in legal_t10_interactive_test_topics:\n",
    "    sub_percentage = 1.0\n",
    "    bound_bt = 30\n",
    "    ita = 1.05\n",
    "    tr = 1.0\n",
    "    _df = TarEvalResultReader(data_name='legal', model_name='scal_sp1.0_sr1.0_tr{}_md2_c1.0_spt{}_bnd{}_min_bktsamplerel_ita{}'.format(tr, sub_percentage, bound_bt, ita), exp_id='1', train_test='test', topic_id=topic_id)\n",
    "    _df['tr'] = tr\n",
    "    _df['spt'] = sub_percentage\n",
    "    _df['bnd'] = bound_bt\n",
    "    _df['ita'] = ita\n",
    "    dfs.append(_df)\n",
    "df = pd.concat(dfs, ignore_index=True)\n",
    "df['reliability'] = df.apply(lambda row:1 if row['recall'] >= row['tr'] else 0, axis=1)\n",
    "df['re'] = np.abs(df['recall'] - df['tr']) / df['tr']\n",
    "descdf = df[show_columns]\n",
    "descdf.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## clef 2017 2018 2019 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "for topic_id in clef_2017_training_topics:\n",
    "    for sub_percentage in [0.8, 1.0]:\n",
    "        for bound_bt in [30, 50, 70, 90, 110]:\n",
    "            for ita in [1.0, 1.05]:\n",
    "                for tr in [0.8, 0.9, 1.0]:\n",
    "                    _df = TarEvalResultReader(data_name='clef2017', model_name='scal_sp1.0_sr1.0_tr{}_md2_c1.0_spt{}_bnd{}_min_bktsamplerel_ita{}'.format(tr, sub_percentage, bound_bt, ita), exp_id='1', train_test='train', topic_id=topic_id)\n",
    "                    _df['tr'] = tr\n",
    "                    _df['spt'] = sub_percentage\n",
    "                    _df['bnd'] = bound_bt\n",
    "                    _df['ita'] = ita\n",
    "                    dfs.append(_df)\n",
    "df = pd.concat(dfs, ignore_index=True)\n",
    "df['reliability'] = df.apply(lambda row:1 if row['recall'] >= row['tr'] else 0, axis=1)\n",
    "df = df.groupby(['tr', 'spt', 'bnd', 'ita']).mean()\n",
    "df[['wss_100', 'loss_er', 'norm_area', 'recall', 'cost', 'num_shown', 'num_docs', 'rels_found', 'num_rels', 'ap', 'NCG@10', 'NCG@100', 'reliability']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### clef 2017 test topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "for topic_id in clef_2017_test_topics:\n",
    "    sub_percentage = 0.8\n",
    "    bound_bt = 50\n",
    "    ita = 1.05\n",
    "    tr = 0.8\n",
    "    _df = TarEvalResultReader(data_name='clef2017', model_name='scal_sp1.0_sr1.0_tr{}_md2_c1.0_spt{}_bnd{}_min_bktsamplerel_ita{}'.format(tr, sub_percentage, bound_bt, ita), exp_id='1', train_test='test', topic_id=topic_id)\n",
    "    _df['tr'] = tr\n",
    "    _df['spt'] = sub_percentage\n",
    "    _df['bnd'] = bound_bt\n",
    "    _df['ita'] = ita\n",
    "    dfs.append(_df)\n",
    "df = pd.concat(dfs, ignore_index=True)\n",
    "df['reliability'] = df.apply(lambda row:1 if row['recall'] >= row['tr'] else 0, axis=1)\n",
    "df['re'] = np.abs(df['recall'] - df['tr']) / df['tr']\n",
    "descdf = df[show_columns]\n",
    "descdf.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "for topic_id in clef_2017_test_topics:\n",
    "    sub_percentage = 0.8\n",
    "    bound_bt = 70\n",
    "    ita = 1.05\n",
    "    tr = 0.9\n",
    "    _df = TarEvalResultReader(data_name='clef2017', model_name='scal_sp1.0_sr1.0_tr{}_md2_c1.0_spt{}_bnd{}_min_bktsamplerel_ita{}'.format(tr, sub_percentage, bound_bt, ita), exp_id='1', train_test='test', topic_id=topic_id)\n",
    "    _df['tr'] = tr\n",
    "    _df['spt'] = sub_percentage\n",
    "    _df['bnd'] = bound_bt\n",
    "    _df['ita'] = ita\n",
    "    dfs.append(_df)\n",
    "df = pd.concat(dfs, ignore_index=True)\n",
    "df['reliability'] = df.apply(lambda row:1 if row['recall'] >= row['tr'] else 0, axis=1)\n",
    "df['re'] = np.abs(df['recall'] - df['tr']) / df['tr']\n",
    "descdf = df[show_columns]\n",
    "descdf.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "for topic_id in clef_2017_test_topics:\n",
    "    sub_percentage = 1.0\n",
    "    bound_bt = 110\n",
    "    ita = 1.05\n",
    "    tr = 1.0\n",
    "    _df = TarEvalResultReader(data_name='clef2017', model_name='scal_sp1.0_sr1.0_tr{}_md2_c1.0_spt{}_bnd{}_min_bktsamplerel_ita{}'.format(tr, sub_percentage, bound_bt, ita), exp_id='1', train_test='test', topic_id=topic_id)\n",
    "    _df['tr'] = tr\n",
    "    _df['spt'] = sub_percentage\n",
    "    _df['bnd'] = bound_bt\n",
    "    _df['ita'] = ita\n",
    "    dfs.append(_df)\n",
    "df = pd.concat(dfs, ignore_index=True)\n",
    "df['reliability'] = df.apply(lambda row:1 if row['recall'] >= row['tr'] else 0, axis=1)\n",
    "df['re'] = np.abs(df['recall'] - df['tr']) / df['tr']\n",
    "descdf = df[show_columns]\n",
    "descdf.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### clef 2018 test topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "for topic_id in clef_2018_topics:\n",
    "    sub_percentage = 0.8\n",
    "    bound_bt = 50\n",
    "    ita = 1.05\n",
    "    tr = 0.8\n",
    "    _df = TarEvalResultReader(data_name='clef2018', model_name='scal_sp1.0_sr1.0_tr{}_md2_c1.0_spt{}_bnd{}_min_bktsamplerel_ita{}'.format(tr, sub_percentage, bound_bt, ita), exp_id='1', train_test='test', topic_id=topic_id)\n",
    "    _df['tr'] = tr\n",
    "    _df['spt'] = sub_percentage\n",
    "    _df['bnd'] = bound_bt\n",
    "    _df['ita'] = ita\n",
    "    dfs.append(_df)\n",
    "df = pd.concat(dfs, ignore_index=True)\n",
    "df['reliability'] = df.apply(lambda row:1 if row['recall'] >= row['tr'] else 0, axis=1)\n",
    "df['re'] = np.abs(df['recall'] - df['tr']) / df['tr']\n",
    "descdf = df[show_columns]\n",
    "descdf.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "for topic_id in clef_2018_topics:\n",
    "    sub_percentage = 0.8\n",
    "    bound_bt = 70\n",
    "    ita = 1.05\n",
    "    tr = 0.9\n",
    "    _df = TarEvalResultReader(data_name='clef2018', model_name='scal_sp1.0_sr1.0_tr{}_md2_c1.0_spt{}_bnd{}_min_bktsamplerel_ita{}'.format(tr, sub_percentage, bound_bt, ita), exp_id='1', train_test='test', topic_id=topic_id)\n",
    "    _df['tr'] = tr\n",
    "    _df['spt'] = sub_percentage\n",
    "    _df['bnd'] = bound_bt\n",
    "    _df['ita'] = ita\n",
    "    dfs.append(_df)\n",
    "df = pd.concat(dfs, ignore_index=True)\n",
    "df['reliability'] = df.apply(lambda row:1 if row['recall'] >= row['tr'] else 0, axis=1)\n",
    "df['re'] = np.abs(df['recall'] - df['tr']) / df['tr']\n",
    "descdf = df[show_columns]\n",
    "descdf.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "for topic_id in clef_2018_topics:\n",
    "    sub_percentage = 1.0\n",
    "    bound_bt = 110\n",
    "    ita = 1.05\n",
    "    tr = 1.0\n",
    "    _df = TarEvalResultReader(data_name='clef2018', model_name='scal_sp1.0_sr1.0_tr{}_md2_c1.0_spt{}_bnd{}_min_bktsamplerel_ita{}'.format(tr, sub_percentage, bound_bt, ita), exp_id='1', train_test='test', topic_id=topic_id)\n",
    "    _df['tr'] = tr\n",
    "    _df['spt'] = sub_percentage\n",
    "    _df['bnd'] = bound_bt\n",
    "    _df['ita'] = ita\n",
    "    dfs.append(_df)\n",
    "df = pd.concat(dfs, ignore_index=True)\n",
    "df['reliability'] = df.apply(lambda row:1 if row['recall'] >= row['tr'] else 0, axis=1)\n",
    "df['re'] = np.abs(df['recall'] - df['tr']) / df['tr']\n",
    "descdf = df[show_columns]\n",
    "descdf.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### clef 2019 test topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "for topic_id in clef_2019_topics:\n",
    "    sub_percentage = 0.8\n",
    "    bound_bt = 50\n",
    "    ita = 1.05\n",
    "    tr = 0.8\n",
    "    _df = TarEvalResultReader(data_name='clef2019', model_name='scal_sp1.0_sr1.0_tr{}_md2_c1.0_spt{}_bnd{}_min_bktsamplerel_ita{}'.format(tr, sub_percentage, bound_bt, ita), exp_id='1', train_test='test', topic_id=topic_id)\n",
    "    _df['tr'] = tr\n",
    "    _df['spt'] = sub_percentage\n",
    "    _df['bnd'] = bound_bt\n",
    "    _df['ita'] = ita\n",
    "    dfs.append(_df)\n",
    "df = pd.concat(dfs, ignore_index=True)\n",
    "df['reliability'] = df.apply(lambda row:1 if row['recall'] >= row['tr'] else 0, axis=1)\n",
    "df['re'] = np.abs(df['recall'] - df['tr']) / df['tr']\n",
    "descdf = df[show_columns]\n",
    "descdf.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "for topic_id in clef_2019_topics:\n",
    "    sub_percentage = 0.8\n",
    "    bound_bt = 70\n",
    "    ita = 1.05\n",
    "    tr = 0.9\n",
    "    _df = TarEvalResultReader(data_name='clef2019', model_name='scal_sp1.0_sr1.0_tr{}_md2_c1.0_spt{}_bnd{}_min_bktsamplerel_ita{}'.format(tr, sub_percentage, bound_bt, ita), exp_id='1', train_test='test', topic_id=topic_id)\n",
    "    _df['tr'] = tr\n",
    "    _df['spt'] = sub_percentage\n",
    "    _df['bnd'] = bound_bt\n",
    "    _df['ita'] = ita\n",
    "    dfs.append(_df)\n",
    "df = pd.concat(dfs, ignore_index=True)\n",
    "df['reliability'] = df.apply(lambda row:1 if row['recall'] >= row['tr'] else 0, axis=1)\n",
    "df['re'] = np.abs(df['recall'] - df['tr']) / df['tr']\n",
    "descdf = df[show_columns]\n",
    "descdf.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "for topic_id in clef_2019_topics:\n",
    "    sub_percentage = 1.0\n",
    "    bound_bt = 110\n",
    "    ita = 1.05\n",
    "    tr = 1.0\n",
    "    _df = TarEvalResultReader(data_name='clef2019', model_name='scal_sp1.0_sr1.0_tr{}_md2_c1.0_spt{}_bnd{}_min_bktsamplerel_ita{}'.format(tr, sub_percentage, bound_bt, ita), exp_id='1', train_test='test', topic_id=topic_id)\n",
    "    _df['tr'] = tr\n",
    "    _df['spt'] = sub_percentage\n",
    "    _df['bnd'] = bound_bt\n",
    "    _df['ita'] = ita\n",
    "    dfs.append(_df)\n",
    "df = pd.concat(dfs, ignore_index=True)\n",
    "df['reliability'] = df.apply(lambda row:1 if row['recall'] >= row['tr'] else 0, axis=1)\n",
    "df['re'] = np.abs(df['recall'] - df['tr']) / df['tr']\n",
    "descdf = df[show_columns]\n",
    "descdf.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# autostop "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## clef 2017 training topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "for topic_id in clef_2017_training_topics:\n",
    "\n",
    "    epsilon = 0.5\n",
    "    beta = -0.8\n",
    "    for target_recall in [0.8, 0.9, 1.0]:\n",
    "        for sampler in ['MixtureUniform', 'PowerLaw', 'APPrior']:\n",
    "            for sampler_type, stop_condition in [('{}Sampler'.format(sampler), 'loose'),\n",
    "                                                ('{}Sampler'.format(sampler), 'strict1'),\n",
    "                                                ('{}Sampler'.format(sampler), 'strict2'),\n",
    "                                                ('{}IPSampler'.format(sampler), 'loose'),\n",
    "                                                ('{}IPSampler'.format(sampler), 'strict1')]:\n",
    "                if sampler_type == 'MixtureUniformSampler' or sampler_type == 'MixtureUniformIPSampler':\n",
    "                    sampler_type += '_' + 'epsilon' + str(epsilon)\n",
    "                elif sampler_type == 'PowerLawSampler' or sampler_type == 'PowerLawIPSampler':\n",
    "                    sampler_type += '_' + 'beta' + str(beta)\n",
    "\n",
    "                _df = TarEvalResultReader(data_name='clef2017', model_name='autostop_cttopicwise_md2_c1.0_sp1.0_sr1.0_smp{}_tr{}_sc{}'.format(sampler_type, target_recall, stop_condition), exp_id='1', train_test='train', topic_id=topic_id)\n",
    "                _df['smp'] = sampler_type\n",
    "                _df['sc'] = stop_condition\n",
    "                _df['tr'] = target_recall\n",
    "                dfs.append(_df)\n",
    "            \n",
    "df = pd.concat(dfs, ignore_index=True)\n",
    "df['reliability'] = df.apply(lambda row:1 if row['recall'] >= row['tr'] else 0, axis=1)\n",
    "df['re'] = np.abs(df['recall'] - df['tr']) / df['tr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_df = df.groupby(['tr', 'smp', 'sc']).mean()\n",
    "group_df[['wss_100', 'loss_er', 'norm_area', 'recall', 'cost', 'num_shown', 'num_docs', 'rels_found', 'num_rels', 'ap', 'NCG@10', 'NCG@100', 'reliability']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value = 'loss_er'\n",
    "target_recall = 1.0\n",
    "group_df = df[df['tr'] == target_recall].groupby(['smp', 'sc']).mean()\n",
    "group_df = group_df.reset_index()\n",
    "\n",
    "group_df.loc[(group_df['smp']=='APPriorSampler'), 'smp']= 'APPrior&HT'\n",
    "group_df.loc[(group_df['smp']=='APPriorIPSampler'), 'smp']= 'APPrior&HH'\n",
    "group_df.loc[(group_df['smp']=='MixtureUniformSampler_epsilon0.5'), 'smp']= 'MixtureUniform&HT'\n",
    "group_df.loc[(group_df['smp']=='MixtureUniformIPSampler_epsilon0.5'), 'smp']= 'MixtureUniform&HH'\n",
    "group_df.loc[(group_df['smp']=='PowerLawSampler_beta-0.8'), 'smp']= 'PowerLaw&HT'\n",
    "group_df.loc[(group_df['smp']=='PowerLawIPSampler_beta-0.8'), 'smp']= 'PowerLaw&HH'\n",
    "group_df.loc[(group_df['sc']=='loose'), 'sc']= 'Opt'\n",
    "group_df.loc[(group_df['sc']=='strict1'), 'sc']= 'Con1'\n",
    "group_df.loc[(group_df['sc']=='strict2'), 'sc']= 'Con2'\n",
    "\n",
    "\n",
    "group_df.rename(columns={'smp':'samplingestimationmethod', 'sc': 'stoppingstrategy'})\n",
    "# heatmap_df = group_df.pivot(\"samplingestimationmethod\", \"stoppingstrategy\", value)\n",
    "heatmap_df = group_df.pivot(\"smp\", \"sc\", value)\n",
    "# heatmap_df.rename(columns={'smp':'Sampling & estimation method', 'sc': 'Stopping strategy'})\n",
    "\n",
    "plt.rcParams['xtick.labelsize'] = 6.5\n",
    "plt.rcParams['ytick.labelsize'] = 6.5\n",
    "plt.rcParams[\"legend.loc\"]='lower right'\n",
    "plt.figure(figsize=(8.27*0.7, 8.27*0.6)) # a4 paper size\n",
    "plt.xlim(0, 1) \n",
    "plt.ylim(0, 1.05) \n",
    "\n",
    "sns_plot = sns.heatmap(heatmap_df, annot=True, fmt=\".3f\", cmap='YlGnBu') \n",
    "sns_plot.set_yticklabels(sns_plot.get_yticklabels(), rotation=45)\n",
    "\n",
    "figure = sns_plot.get_figure() \n",
    "figure.savefig(os.path.join(paperfiguredir, 'component-{}-{}.pdf'.format(value.replace('_', ''), int(target_recall*100))))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## clef 2017 test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "for topic_id in clef_2017_test_topics:\n",
    "    for target_recall in [1.0, 0.9, 0.8]:\n",
    "        sampler_type = 'APPriorSampler'\n",
    "        stop_condition = 'strict1'\n",
    "        try:\n",
    "            _df = TarEvalResultReader(data_name='clef2017', model_name='autostop_cttopicwise_md2_c1.0_sp1.0_sr1.0_smp{}_tr{}_sc{}'.format(sampler_type, target_recall, stop_condition), exp_id='1', train_test='test', topic_id=topic_id)\n",
    "            _df['smp'] = sampler_type\n",
    "            _df['sc'] = stop_condition\n",
    "            _df['tr'] = target_recall\n",
    "            dfs.append(_df)\n",
    "        except:\n",
    "            print(topic_id)\n",
    "            \n",
    "df = pd.concat(dfs, ignore_index=True)\n",
    "df['reliability'] = df.apply(lambda row:1 if row['recall'] >= row['tr'] else 0, axis=1)\n",
    "df['re'] = np.abs(df['recall'] - df['tr']) / df['tr']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descdf = df[df['tr'] == 0.8][show_columns]\n",
    "descdf.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descdf = df[df['tr'] == 0.9][show_columns]\n",
    "descdf.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descdf = df[df['tr'] == 1.0][show_columns]\n",
    "descdf.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## clef 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "for topic_id in clef_2018_topics:\n",
    "    for target_recall in [1.0, 0.9, 0.8]:\n",
    "        sampler_type = 'APPriorSampler'\n",
    "        stop_condition = 'strict1'\n",
    "        try:\n",
    "            _df = TarEvalResultReader(data_name='clef2018', model_name='autostop_cttopicwise_md2_c1.0_sp1.0_sr1.0_smp{}_tr{}_sc{}'.format(sampler_type, target_recall, stop_condition), exp_id='1', train_test='test', topic_id=topic_id)\n",
    "            _df['smp'] = sampler_type\n",
    "            _df['sc'] = stop_condition\n",
    "            _df['tr'] = target_recall\n",
    "            dfs.append(_df)\n",
    "        except:\n",
    "            print(topic_id)\n",
    "            \n",
    "df = pd.concat(dfs, ignore_index=True)\n",
    "df['reliability'] = df.apply(lambda row:1 if row['recall'] >= row['tr'] else 0, axis=1)\n",
    "df['re'] = np.abs(df['recall'] - df['tr']) / df['tr']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descdf = df[df['tr'] == 0.8][show_columns]\n",
    "descdf.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descdf = df[df['tr'] == 0.9][show_columns]\n",
    "descdf.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descdf = df[df['tr'] == 1.0][show_columns]\n",
    "descdf.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## clef 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "for topic_id in clef_2019_topics:\n",
    "    for target_recall in [1.0, 0.9, 0.8]:\n",
    "        sampler_type = 'APPriorSampler'\n",
    "        stop_condition = 'strict1'\n",
    "        try:\n",
    "            _df = TarEvalResultReader(data_name='clef2019', model_name='autostop_cttopicwise_md2_c1.0_sp1.0_sr1.0_smp{}_tr{}_sc{}'.format(sampler_type, target_recall, stop_condition), exp_id='1', train_test='test', topic_id=topic_id)\n",
    "            _df['smp'] = sampler_type\n",
    "            _df['sc'] = stop_condition\n",
    "            _df['tr'] = target_recall\n",
    "            dfs.append(_df)\n",
    "        except:\n",
    "            print(topic_id)\n",
    "            \n",
    "df = pd.concat(dfs, ignore_index=True)\n",
    "df['reliability'] = df.apply(lambda row:1 if row['recall'] >= row['tr'] else 0, axis=1)\n",
    "df['re'] = np.abs(df['recall'] - df['tr']) / df['tr']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descdf = df[df['tr'] == 0.8][show_columns]\n",
    "descdf.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descdf = df[df['tr'] == 0.9][show_columns]\n",
    "descdf.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descdf = df[df['tr'] == 1.0][show_columns]\n",
    "descdf.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## athome4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "for topic_id in athome4_topics:\n",
    "    for target_recall in [1.0, 0.9, 0.8]:\n",
    "        sampler_type = 'APPriorSampler'\n",
    "        stop_condition = 'strict1'\n",
    "        try:\n",
    "            _df = TarEvalResultReader(data_name='athome4', model_name='autostoplarge_cttopicwise_md2_c1.0_sp1.0_sr1.0_smp{}_tr{}_sc{}'.format(sampler_type, target_recall, stop_condition), exp_id='1', train_test='test', topic_id=topic_id)\n",
    "            _df['smp'] = sampler_type\n",
    "            _df['sc'] = stop_condition\n",
    "            _df['tr'] = target_recall\n",
    "            dfs.append(_df)\n",
    "        except:\n",
    "            print(topic_id)\n",
    "            \n",
    "df = pd.concat(dfs, ignore_index=True)\n",
    "df['reliability'] = df.apply(lambda row:1 if row['recall'] >= row['tr'] else 0, axis=1)\n",
    "df['re'] = np.abs(df['recall'] - df['tr']) / df['tr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descdf = df[df['tr'] == 0.8][show_columns]\n",
    "descdf.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descdf = df[df['tr'] == 0.9][show_columns]\n",
    "descdf.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descdf = df[df['tr'] == 1.0][show_columns]\n",
    "descdf.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## legal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "for topic_id in legal_t10_interactive_test_topics:\n",
    "    for target_recall in [1.0, 0.9, 0.8]:\n",
    "        sampler_type = 'APPriorSampler'\n",
    "        stop_condition = 'strict1'\n",
    "        try:\n",
    "            _df = TarEvalResultReader(data_name='legal', model_name='autostoplarge_cttopicwise_md2_c1.0_sp1.0_sr1.0_smp{}_tr{}_sc{}'.format(sampler_type, target_recall, stop_condition), exp_id='1', train_test='test', topic_id=topic_id)\n",
    "            _df['smp'] = sampler_type\n",
    "            _df['sc'] = stop_condition\n",
    "            _df['tr'] = target_recall\n",
    "            dfs.append(_df)\n",
    "        except:\n",
    "            print(topic_id)\n",
    "            \n",
    "df = pd.concat(dfs, ignore_index=True)\n",
    "df['reliability'] = df.apply(lambda row:1 if row['recall'] >= row['tr'] else 0, axis=1)\n",
    "df['re'] = np.abs(df['recall'] - df['tr']) / df['tr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descdf = df[df['tr'] == 0.8][show_columns]\n",
    "descdf.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descdf = df[df['tr'] == 0.9][show_columns]\n",
    "descdf.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descdf = df[df['tr'] == 1.0][show_columns]\n",
    "descdf.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "data_name = 'clef2019'\n",
    "target_recall = 1.0\n",
    "tr = '08'\n",
    "sampler = 'APPrior'\n",
    "exp_ids = ['1']\n",
    "topic_ids =  clef_2019_topics # set(clef_2017_topics) - set(example_topics) - set(['CD008643']) # set(athome4_topics) - set(['415'])\n",
    "\n",
    "folders = [\n",
    "'knee_sp1.0_sr1.0_rho6',\n",
    "'scal_tr{}_'.format(target_recall),\n",
    "'random_sp1.0_sr1.0_tr{}'.format(target_recall),\n",
    "'autostop_sp1.0_sr1.0_update_sampler{}Sampler_auto_tr{}_scloose'.format(sampler, target_recall),\n",
    "'autostop_sp1.0_sr1.0_update_sampler{}Sampler_auto_tr{}_scstrict1'.format(sampler, target_recall),\n",
    "'autostop_sp1.0_sr1.0_update_sampler{}Sampler_auto_tr{}_scstrict2'.format(sampler, target_recall),\n",
    "'autostop_sp1.0_sr1.0_update_sampler{}IPSampler_auto_tr{}_scloose'.format(sampler, target_recall),\n",
    "'autostop_sp1.0_sr1.0_update_sampler{}IPSampler_auto_tr{}_scstrict1'.format(sampler, target_recall),\n",
    "]\n",
    "\n",
    "df1 = load_data(data_name=data_name, folders=folders, exp_ids=exp_ids, topic_ids=topic_ids, func=TarEvalResultReader)\n",
    "df2 = load_data(data_name=data_name, folders=folders, exp_ids=exp_ids, topic_ids=topic_ids, func=InteractionLastResult)\n",
    "df2 = df2[['model_name', 'exp_id', 'topic_id', 'ap', 'running_true_r', 'total_true_r']]\n",
    "\n",
    "df = pd.merge(df1, df2, on=['model_name', 'exp_id', 'topic_id'])\n",
    "df['normed_sampled_num'] = df['sampled_num']/df['total_num']\n",
    "df['relative_error'] = np.abs(df['running_true_r'] - df['total_true_r']) / df['total_true_r']\n",
    "df['relative_error_s'] = np.abs(df['true_stopped_recall'] - target_recall) / target_recall\n",
    "\n",
    "df_mean = df.groupby(['model_name']).mean()\n",
    "df_mean = df_mean.reset_index()\n",
    "df_mean = df_mean.round(2)\n",
    "df_mean = df_mean[['model_name', 'normed_sampled_num', 'true_stopped_recall', 'ap', 'relative_error', 'relative_error_s','losser']]\n",
    "df_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "knee = df[df['model_name'] == '2-scal']\n",
    "autostop = df[df['model_name'] == '4-strict1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "knee[['losser']].plot(kind='box')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "diff = knee['losser'].values - autostop['losser'].values\n",
    "tempdf = pd.DataFrame(diff)\n",
    "\n",
    "tempdf.plot(kind='hist')\n",
    "\n",
    "from scipy import stats\n",
    "stats.shapiro(diff)\n",
    "\n",
    "stats.ttest_rel(knee['losser'].values, autostop['losser'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "df1 = load_data(data_name=data_name, folders=folders, exp_ids=exp_ids, topic_ids=topic_ids, func=TarEvalResultReader)\n",
    "df2 = load_data(data_name=data_name, folders=folders, exp_ids=exp_ids, topic_ids=topic_ids, func=InteractionLastResult)\n",
    "df2 = df2[['model_name', 'exp_id', 'topic_id', 'ap', 'running_true_r', 'total_true_r']]\n",
    "\n",
    "df = pd.merge(df1, df2, on=['model_name', 'exp_id', 'topic_id'])\n",
    "df['normed_sampled_num'] = df['sampled_num']/df['total_num']\n",
    "df['relative_error'] = np.abs(df['running_true_r'] - df['total_true_r']) / df['total_true_r']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "df_mean = df.groupby(['model_name']).mean()\n",
    "df_mean = df_mean.reset_index()\n",
    "df_mean = df_mean.round(2)\n",
    "df_mean = df_mean[['model_name', 'normed_sampled_num', 'true_stopped_recall', 'ap', 'relative_error', 'losser']]\n",
    "df_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recall_cost v.s. Cost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def plot_cost_recall(data_name, topic_ids, target_recall, beta, rho, target_rel_num, sub_percentage, bound_bt, ita, sample_percentage, sampler):\n",
    "    exp_ids = ['1']\n",
    "    sd_target_recall = 0.99 if target_recall == 1.0 else target_recall\n",
    "    autostop = 'autostoplarge' if 'athome' in data_name or 'legal' in data_name else 'autostop'\n",
    "    \n",
    "    if target_recall == 1.0:\n",
    "        folders = [\n",
    "        'knee_sb{}_sp1.0_sr1.0_rho{}_cttopicwise_md2_c1.0'.format(beta, rho),\n",
    "        'target_sp1.0_sr1.0_md2_c1.0_trn{}'.format(target_rel_num),\n",
    "        'scal_sp1.0_sr1.0_tr{}_md2_c1.0_spt{}_bnd{}_min_bktsamplerel_ita{}'.format(target_recall, sub_percentage, bound_bt, ita),\n",
    "        'sdtf_tr{}'.format(sd_target_recall),\n",
    "        'sdfu_smp{}_tr{}'.format(sample_percentage, sd_target_recall),\n",
    "        '{}_cttopicwise_md2_c1.0_sp1.0_sr1.0_smp{}Sampler_tr{}_scstrict1'.format(autostop, sampler, target_recall),\n",
    "        ]\n",
    "        markers = ['X', '*', 'o', 'D', 's', 'v']\n",
    "#         labels = ['', 'Knee', 'Target', 'SCAL', 'SD-training', 'SD-sampling', 'Ours'] \n",
    "    else:\n",
    "        folders = [\n",
    "        'scal_sp1.0_sr1.0_tr{}_md2_c1.0_spt{}_bnd{}_min_bktsamplerel_ita{}'.format(target_recall, sub_percentage, bound_bt, ita),\n",
    "        'sdtf_tr{}'.format(sd_target_recall),\n",
    "        'sdfu_smp{}_tr{}'.format(sample_percentage, sd_target_recall),\n",
    "        '{}_cttopicwise_md2_c1.0_sp1.0_sr1.0_smp{}Sampler_tr{}_scstrict1'.format(autostop, sampler, target_recall),\n",
    "        ]\n",
    "        markers = ['o', 'D', 's', 'v']\n",
    "#         labels = ['', 'SCAL', 'SD-training', 'SD-sampling', 'Ours'] \n",
    "        \n",
    "    df = load_data(data_name=data_name, folders=folders, exp_ids=exp_ids, topic_ids=topic_ids, func=TarEvalResultReader)\n",
    "\n",
    "    df_mean = df.groupby(['model_name']).mean()\n",
    "    df_mean = df_mean.reset_index()\n",
    "    df_mean = df_mean.round(2)\n",
    "    df_mean = df_mean[['model_name', 'cost', 'recall', 'loss_er']]\n",
    "    \n",
    "    params = {\n",
    "            'xtick.labelsize': 14,\n",
    "             'ytick.labelsize': 14,\n",
    "        'axes.labelsize': 14,\n",
    "             'legend.loc': 'lower right',\n",
    "            'legend.fontsize': 14,\n",
    "          'legend.handlelength': 1}\n",
    "    matplotlib.rcParams.update(params)\n",
    "\n",
    "    plt.figure(figsize=(8.27*0.5, 8.27*0.5)) # a4 paper size\n",
    "    plt.xlim(0, 1.05) \n",
    "    plt.ylim(0, 1.05) \n",
    "    \n",
    "    sns_plot = sns.scatterplot(x='cost', y='recall', hue='model_name', style='model_name', \n",
    "                               markers=markers, edgecolor='none', alpha=0.9, data=df_mean, legend='brief')\n",
    "\n",
    "    p_frontX, p_frontY = pareto_frontier(df_mean['cost'].values, df_mean['recall'].values, maxX=False)\n",
    "    sns_plot = sns.lineplot(p_frontX, p_frontY, color='grey', linewidth='1')\n",
    "    \n",
    "    sns_plot.xaxis.set_major_locator(plt.MaxNLocator(6))\n",
    "    sns_plot.yaxis.set_major_locator(plt.MaxNLocator(6))\n",
    "    sns_plot.set_xlabel('Cost')\n",
    "    sns_plot.set_ylabel('$Recall$')\n",
    "#     handles, _ = sns_plot.get_legend_handles_labels()\n",
    "#     sns_plot.legend(handles, labels)\n",
    "\n",
    "    figure = sns_plot.get_figure() \n",
    "    figure.savefig(os.path.join(paperfiguredir, '{}-tr{}-pareto.pdf'.format(data_name, int(target_recall*10))), bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_name = 'clef2019'\n",
    "topic_ids = clef_2019_test_topics \n",
    "target_recall = 0.8\n",
    "beta = 1000.0\n",
    "rho = 'dynamic'\n",
    "target_rel_num = 15\n",
    "sub_percentage = 0.8\n",
    "bound_bt = 50\n",
    "ita = 1.05\n",
    "sample_percentage = 0.4\n",
    "sampler = 'APPrior'\n",
    "plot_cost_recall(data_name, topic_ids, target_recall, beta, rho, target_rel_num, sub_percentage, bound_bt, ita, sample_percentage, sampler)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "data_name = 'clef2019'\n",
    "topic_ids = clef_2019_test_topics \n",
    "target_recall = 0.9\n",
    "beta = 1000.0\n",
    "rho = 'dynamic'\n",
    "target_rel_num = 15\n",
    "sub_percentage = 0.8\n",
    "bound_bt = 70\n",
    "ita = 1.05\n",
    "sample_percentage = 0.3\n",
    "sampler = 'APPrior'\n",
    "\n",
    "plot_cost_recall(data_name, topic_ids, target_recall, beta, rho, target_rel_num, sub_percentage, bound_bt, ita, sample_percentage, sampler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_name = 'clef2019'\n",
    "topic_ids = clef_2019_test_topics \n",
    "target_recall = 1.0\n",
    "beta = 1000.0\n",
    "rho = 'dynamic'\n",
    "target_rel_num = 15\n",
    "sub_percentage = 1.0\n",
    "bound_bt = 110\n",
    "ita = 1.05\n",
    "sample_percentage = 0.2\n",
    "sampler = 'APPrior'\n",
    "plot_cost_recall(data_name, topic_ids, target_recall, beta, rho, target_rel_num, sub_percentage, bound_bt, ita, sample_percentage, sampler)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_name = 'athome4'\n",
    "topic_ids = athome4_topics \n",
    "target_recall = 0.8\n",
    "beta = 1000.0\n",
    "rho = 10\n",
    "target_rel_num = 15\n",
    "sub_percentage = 0.8\n",
    "bound_bt = 70\n",
    "ita = 1.05\n",
    "sample_percentage = 0.1\n",
    "sampler = 'APPrior'\n",
    "plot_cost_recall(data_name, topic_ids, target_recall, beta, rho, target_rel_num, sub_percentage, bound_bt, ita, sample_percentage, sampler)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_name = 'athome4'\n",
    "topic_ids = athome4_topics \n",
    "target_recall = 0.9\n",
    "beta = 1000.0\n",
    "rho = 10\n",
    "target_rel_num = 15\n",
    "sub_percentage = 1.0\n",
    "bound_bt = 110\n",
    "ita = 1.05\n",
    "sample_percentage = 0.1\n",
    "sampler = 'APPrior'\n",
    "plot_cost_recall(data_name, topic_ids, target_recall, beta, rho, target_rel_num, sub_percentage, bound_bt, ita, sample_percentage, sampler)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_name = 'athome4'\n",
    "topic_ids = athome4_topics \n",
    "target_recall = 1.0\n",
    "beta = 1000.0\n",
    "rho = 10\n",
    "target_rel_num = 15\n",
    "sub_percentage = 1.0\n",
    "bound_bt = 110\n",
    "ita = 1.05\n",
    "sample_percentage = 0.1\n",
    "sampler = 'APPrior'\n",
    "plot_cost_recall(data_name, topic_ids, target_recall, beta, rho, target_rel_num, sub_percentage, bound_bt, ita, sample_percentage, sampler)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_name = 'legal'\n",
    "topic_ids = legal_t10_interactive_test_topics \n",
    "target_recall = 1.0\n",
    "beta = 1000.0\n",
    "rho = 'dynamic'\n",
    "target_rel_num = 10\n",
    "sub_percentage = 1.0\n",
    "bound_bt = 30\n",
    "ita = 1.05\n",
    "sample_percentage = 0.1\n",
    "sampler = 'APPrior'\n",
    "plot_cost_recall(data_name, topic_ids, target_recall, beta, rho, target_rel_num, sub_percentage, bound_bt, ita, sample_percentage, sampler)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_name = 'legal'\n",
    "topic_ids = legal_t10_interactive_test_topics \n",
    "target_recall = 0.9\n",
    "beta = 1000.0\n",
    "rho = 'dynamic'\n",
    "target_rel_num = 10\n",
    "sub_percentage = 1.0\n",
    "bound_bt = 30\n",
    "ita = 1.05\n",
    "sample_percentage = 0.1\n",
    "sampler = 'APPrior'\n",
    "plot_cost_recall(data_name, topic_ids, target_recall, beta, rho, target_rel_num, sub_percentage, bound_bt, ita, sample_percentage, sampler)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_name = 'legal'\n",
    "topic_ids = legal_t10_interactive_test_topics \n",
    "target_recall = 0.8\n",
    "beta = 1000.0\n",
    "rho = 'dynamic'\n",
    "target_rel_num = 10\n",
    "sub_percentage = 1.0\n",
    "bound_bt = 30\n",
    "ita = 1.05\n",
    "sample_percentage = 0.1\n",
    "sampler = 'APPrior'\n",
    "plot_cost_recall(data_name, topic_ids, target_recall, beta, rho, target_rel_num, sub_percentage, bound_bt, ita, sample_percentage, sampler)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# topic-wise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def plot_cost_recall_topicwise(data_name, topic_ids, target_recall, beta, rho, target_rel_num, sub_percentage, bound_bt, ita, sample_percentage, sampler):\n",
    "    exp_ids = ['1']\n",
    "    sd_target_recall = 0.99 if target_recall == 1.0 else target_recall\n",
    "    autostop = 'autostoplarge' if 'athome' in data_name or 'legal' in data_name else 'autostop'\n",
    "    \n",
    "    if target_recall == 1.0:\n",
    "        folders = [\n",
    "        'knee_sb{}_sp1.0_sr1.0_rho{}_cttopicwise_md2_c1.0'.format(beta, rho),\n",
    "        'target_sp1.0_sr1.0_md2_c1.0_trn{}'.format(target_rel_num),\n",
    "        'scal_sp1.0_sr1.0_tr{}_md2_c1.0_spt{}_bnd{}_min_bktsamplerel_ita{}'.format(target_recall, sub_percentage, bound_bt, ita),\n",
    "        'sdtf_tr{}'.format(sd_target_recall),\n",
    "        'sdfu_smp{}_tr{}'.format(sample_percentage, sd_target_recall),\n",
    "        '{}_cttopicwise_md2_c1.0_sp1.0_sr1.0_smp{}Sampler_tr{}_scstrict1'.format(autostop, sampler, target_recall),\n",
    "        ]\n",
    "        markers = ['X', '*', 'o', 'D', 's', 'v']\n",
    "#         labels = ['', 'Knee', 'Target', 'SCAL', 'SD-training', 'SD-sampling', 'Ours'] \n",
    "    else:\n",
    "        folders = [\n",
    "        'scal_sp1.0_sr1.0_tr{}_md2_c1.0_spt{}_bnd{}_min_bktsamplerel_ita{}'.format(target_recall, sub_percentage, bound_bt, ita),\n",
    "        'sdtf_tr{}'.format(sd_target_recall),\n",
    "        'sdfu_smp{}_tr{}'.format(sample_percentage, sd_target_recall),\n",
    "        '{}_cttopicwise_md2_c1.0_sp1.0_sr1.0_smp{}Sampler_tr{}_scstrict1'.format(autostop, sampler, target_recall),\n",
    "        ]\n",
    "        markers = ['o', 'D', 's', 'v']\n",
    "#         labels = ['', 'SCAL', 'SD-training', 'SD-sampling', 'Ours'] \n",
    "        \n",
    "    df = load_data(data_name=data_name, folders=folders, exp_ids=exp_ids, topic_ids=topic_ids, func=TarEvalResultReader)\n",
    "\n",
    "#     df_mean = df.groupby(['model_name']).mean()\n",
    "#     df_mean = df_mean.reset_index()\n",
    "#     df_mean = df_mean.round(2)\n",
    "#     df_mean = df_mean[['model_name', 'cost', 'recall', 'loss_er']]\n",
    "\n",
    "    params = {\n",
    "        'xtick.labelsize': 14,\n",
    "         'ytick.labelsize': 14,\n",
    "        'axes.labelsize': 14,\n",
    "         'legend.loc': 'lower right',\n",
    "        'legend.fontsize': 14,\n",
    "      'legend.handlelength': 1}\n",
    "    matplotlib.rcParams.update(params)\n",
    "    \n",
    "    plt.figure(figsize=(8.27*0.5, 8.27*0.5)) # a4 paper size\n",
    "    plt.xlim(0, 1.05) \n",
    "    plt.ylim(0, 1.05) \n",
    "    sns_plot = sns.scatterplot(x='cost', y='recall', hue='model_name', style='model_name', \n",
    "                               markers=markers, edgecolor='none', alpha=0.8, s=26, data=df, legend='brief')\n",
    "\n",
    "#     p_frontX, p_frontY = pareto_frontier(df_mean['cost'].values, df_mean['recall'].values, maxX=False)\n",
    "#     sns_plot = sns.lineplot(p_frontX, p_frontY, color='grey', linewidth='1')\n",
    "    sns_plot.xaxis.set_major_locator(plt.MaxNLocator(6))\n",
    "    sns_plot.yaxis.set_major_locator(plt.MaxNLocator(6))\n",
    "    sns_plot.set_xlabel('Cost')\n",
    "    sns_plot.set_ylabel('$Recall$')\n",
    "#     handles, _ = sns_plot.get_legend_handles_labels()\n",
    "#     sns_plot.legend(handles, labels)\n",
    "\n",
    "    figure = sns_plot.get_figure() \n",
    "    figure.savefig(os.path.join(paperfiguredir, '{}-tr{}-pareto-topicwise.pdf'.format(data_name, int(target_recall*100))), bbox_inches='tight')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "for data_name, topic_ids in[('clef2017',clef_2017_test_topics),\n",
    "                            ('clef2018',clef_2018_topics),\n",
    "                            ('clef2019',clef_2019_topics)]:\n",
    "\n",
    "    target_recall = 0.8\n",
    "    beta = 1000.0\n",
    "    rho = 'dynamic'\n",
    "    target_rel_num = 15\n",
    "    sub_percentage = 0.8\n",
    "    bound_bt = 50\n",
    "    ita = 1.05\n",
    "    sample_percentage = 0.4\n",
    "    sampler = 'APPrior'\n",
    "    plot_cost_recall_topicwise(data_name, topic_ids, target_recall, beta, rho, target_rel_num, sub_percentage, bound_bt, ita, sample_percentage, sampler)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "for data_name, topic_ids in[('clef2017',clef_2017_test_topics),\n",
    "                            ('clef2018',clef_2018_topics),\n",
    "                            ('clef2019',clef_2019_topics)]: \n",
    "    target_recall = 0.9\n",
    "    beta = 1000.0\n",
    "    rho = 'dynamic'\n",
    "    target_rel_num = 15\n",
    "    sub_percentage = 0.8\n",
    "    bound_bt = 70\n",
    "    ita = 1.05\n",
    "    sample_percentage = 0.3\n",
    "    sampler = 'APPrior'\n",
    "\n",
    "\n",
    "    plot_cost_recall_topicwise(data_name, topic_ids, target_recall, beta, rho, target_rel_num, sub_percentage, bound_bt, ita, sample_percentage, sampler)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "for data_name, topic_ids in[('clef2017',clef_2017_test_topics),\n",
    "                            ('clef2018',clef_2018_topics),\n",
    "                            ('clef2019',clef_2019_topics)]:\n",
    "    target_recall = 1.0\n",
    "    beta = 1000.0\n",
    "    rho = 'dynamic'\n",
    "    target_rel_num = 15\n",
    "    sub_percentage = 1.0\n",
    "    bound_bt = 110\n",
    "    ita = 1.05\n",
    "    sample_percentage = 0.2\n",
    "    sampler = 'APPrior'\n",
    "\n",
    "    plot_cost_recall_topicwise(data_name, topic_ids, target_recall, beta, rho, target_rel_num, sub_percentage, bound_bt, ita, sample_percentage, sampler)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "data_name = 'athome4'\n",
    "topic_ids = athome4_topics \n",
    "target_recall = 0.8\n",
    "beta = 1000.0\n",
    "rho = 10\n",
    "target_rel_num = 15\n",
    "sub_percentage = 0.8\n",
    "bound_bt = 70\n",
    "ita = 1.05\n",
    "sample_percentage = 0.1\n",
    "sampler = 'APPrior'\n",
    "plot_cost_recall_topicwise(data_name, topic_ids, target_recall, beta, rho, target_rel_num, sub_percentage, bound_bt, ita, sample_percentage, sampler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "data_name = 'athome4'\n",
    "topic_ids = athome4_topics \n",
    "target_recall = 0.9\n",
    "beta = 1000.0\n",
    "rho = 10\n",
    "target_rel_num = 15\n",
    "sub_percentage = 1.0\n",
    "bound_bt = 110\n",
    "ita = 1.05\n",
    "sample_percentage = 0.1\n",
    "sampler = 'APPrior'\n",
    "plot_cost_recall_topicwise(data_name, topic_ids, target_recall, beta, rho, target_rel_num, sub_percentage, bound_bt, ita, sample_percentage, sampler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_name = 'athome4'\n",
    "topic_ids = athome4_topics \n",
    "target_recall = 1.0\n",
    "beta = 1000.0\n",
    "rho = 10\n",
    "target_rel_num = 15\n",
    "sub_percentage = 1.0\n",
    "bound_bt = 110\n",
    "ita = 1.05\n",
    "sample_percentage = 0.1\n",
    "sampler = 'APPrior'\n",
    "plot_cost_recall_topicwise(data_name, topic_ids, target_recall, beta, rho, target_rel_num, sub_percentage, bound_bt, ita, sample_percentage, sampler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_name = 'legal'\n",
    "topic_ids = legal_t10_interactive_test_topics \n",
    "target_recall = 0.8\n",
    "beta = 1000.0\n",
    "rho = 'dynamic'\n",
    "target_rel_num = 10\n",
    "sub_percentage = 1.0\n",
    "bound_bt = 30\n",
    "ita = 1.05\n",
    "sample_percentage = 0.1\n",
    "sampler = 'APPrior'\n",
    "plot_cost_recall_topicwise(data_name, topic_ids, target_recall, beta, rho, target_rel_num, sub_percentage, bound_bt, ita, sample_percentage, sampler)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_name = 'legal'\n",
    "topic_ids = legal_t10_interactive_test_topics \n",
    "target_recall = 0.9\n",
    "beta = 1000.0\n",
    "rho = 'dynamic'\n",
    "target_rel_num = 10\n",
    "sub_percentage = 1.0\n",
    "bound_bt = 30\n",
    "ita = 1.05\n",
    "sample_percentage = 0.1\n",
    "sampler = 'APPrior'\n",
    "plot_cost_recall_topicwise(data_name, topic_ids, target_recall, beta, rho, target_rel_num, sub_percentage, bound_bt, ita, sample_percentage, sampler)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_name = 'legal'\n",
    "topic_ids = legal_t10_interactive_test_topics \n",
    "target_recall = 1.0\n",
    "beta = 1000.0\n",
    "rho = 'dynamic'\n",
    "target_rel_num = 10\n",
    "sub_percentage = 1.0\n",
    "bound_bt = 30\n",
    "ita = 1.05\n",
    "sample_percentage = 0.1\n",
    "sampler = 'APPrior'\n",
    "plot_cost_recall_topicwise(data_name, topic_ids, target_recall, beta, rho, target_rel_num, sub_percentage, bound_bt, ita, sample_percentage, sampler)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topicwise $R$ v.s. $\\widehat{R}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def plot_truer_estir_topicwise(data_name, topic_ids, target_recall, beta, rho, target_rel_num, sub_percentage, bound_bt, ita, sample_percentage, sampler):\n",
    "    exp_ids = ['1']\n",
    "    sd_target_recall = 0.99 if target_recall == 1.0 else target_recall\n",
    "\n",
    "    folders = [\n",
    "    'scal_sp1.0_sr1.0_tr{}_md2_c1.0_spt{}_bnd{}_min_bktsamplerel_ita{}'.format(target_recall, sub_percentage, bound_bt, ita),\n",
    "    'autostop_cttopicwise_md2_c1.0_sp1.0_sr1.0_smp{}Sampler_tr{}_scstrict1'.format(sampler, target_recall),\n",
    "    ]\n",
    "    markers = ['X', '*']\n",
    "    \n",
    "    def revise_scal(x):\n",
    "        total_esti_r = x['total_esti_r']\n",
    "        if x['model_name'] == 'SCAL':\n",
    "            total_esti_r *= ita\n",
    "        return total_esti_r\n",
    "    \n",
    "#     def calculate_bias_crossrate(df, model_name):\n",
    "#         dff = df[df['model_name'] == model_name]\n",
    "#         seris = dff['revised_total_esti_r'] - dff['total_true_r']\n",
    "#         bias = seris.mean()\n",
    "        \n",
    "#         total_cnt = len(dff)\n",
    "        \n",
    "#         cross_cnt1 = len(dff[(dff['total_true_r'] <= dff['upper_bound']) &\n",
    "#                              (dff['total_true_r'] >= dff['lower_bound'])])\n",
    "#         cross_rate1 = cross_cnt1 / total_cnt\n",
    "        \n",
    "#         cross_cnt2 = len(dff[(dff['total_true_r'] <= dff['revised_total_esti_r'] +  2*np.sqrt(dff['var1'])) &\n",
    "#                              (dff['total_true_r'] >= dff['revised_total_esti_r'] -  2*np.sqrt(dff['var1']))])\n",
    "#         cross_rate2 = cross_cnt1 / total_cnt\n",
    "        \n",
    "#         cross_cnt3 = len(dff[(dff['total_true_r'] <= dff['revised_total_esti_r'] +  3*np.sqrt(dff['var1'])) &\n",
    "#                              (dff['total_true_r'] >= dff['revised_total_esti_r'] -  3*np.sqrt(dff['var1']))])\n",
    "#         cross_rate3 = cross_cnt3 / total_cnt\n",
    "        \n",
    "#         return round(bias, 2), int(cross_rate1*100), int(cross_rate2*100), int(cross_rate3*100)\n",
    "    \n",
    "    def calculate_mse(df, model_name):\n",
    "        dff = df[df['model_name'] == model_name]\n",
    "        seris = (dff['revised_total_esti_r'] - dff['total_true_r'])**2\n",
    "        mse = seris.mean()\n",
    "      \n",
    "        return round(mse, 2)\n",
    "    \n",
    "        \n",
    "    df = load_data(data_name=data_name, folders=folders, exp_ids=exp_ids, topic_ids=topic_ids, func=InteractionLastResult)\n",
    "\n",
    "    df['revised_total_esti_r'] = df.apply(revise_scal, axis=1)\n",
    "    df.loc[df['var1'] <= 0, 'var1'] = 0 \n",
    "    df['lower_bound'] = df['revised_total_esti_r'] -  np.sqrt(df['var1'])\n",
    "    df['upper_bound'] = df['revised_total_esti_r'] +  np.sqrt(df['var1'])\n",
    "    \n",
    "    \n",
    "    params = {\n",
    "        'xtick.labelsize': 14,\n",
    "         'ytick.labelsize': 14,\n",
    "        'axes.labelsize': 14,\n",
    "         'legend.loc': 'lower right',\n",
    "        'legend.fontsize': 14,\n",
    "      'legend.handlelength': 1}\n",
    "    matplotlib.rcParams.update(params)\n",
    "    \n",
    "    plt.figure(figsize=(8.27*0.5, 8.27*0.5)) \n",
    "    plt.xlim(-10, 240) \n",
    "    plt.ylim(-10, 240) \n",
    "    sns_plot=sns.scatterplot(x='total_true_r', y='revised_total_esti_r', hue='model_name',style='model_name', \n",
    "                             markers=['*', '.'], edgecolor='none', alpha=0.8, s=80, data=df)\n",
    "    sns_plot=sns.lineplot([-10, 550], [-10, 550], linewidth= 0.2, dashes=True, color='grey')\n",
    "\n",
    "    plt.vlines(x=df[df['model_name'] == 'Ours']['total_true_r'].values, \n",
    "               ymin=df[df['model_name'] == 'Ours']['lower_bound'].values, \n",
    "               ymax=df[df['model_name'] == 'Ours']['upper_bound'].values, color='orange', linewidth=0.5)\n",
    "\n",
    "\n",
    "    mse = calculate_mse(df, 'Ours')\n",
    "    mse_ = calculate_mse(df, 'SCAL')\n",
    "    sns_plot.text(1,230, 'MSE:{} (Ours), {} (SCAL)'.format(mse, mse_), fontsize=12) \n",
    "    \n",
    "    \n",
    "    sns_plot.set_xlabel('R')\n",
    "    sns_plot.set_ylabel('$\\widehat{R}$', rotation=0)\n",
    "    figure = sns_plot.get_figure() \n",
    "\n",
    "\n",
    "    figure.savefig(os.path.join(paperfiguredir, '{}-tr{}-truer-estir-topicwise.pdf'.format(data_name, int(target_recall*100))), bbox_inches='tight')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_name = 'clef2017'\n",
    "topic_ids = clef_2017_test_topics \n",
    "target_recall = 0.8\n",
    "beta = 1000.0\n",
    "rho = 'dynamic'\n",
    "target_rel_num = 15\n",
    "sub_percentage = 0.8\n",
    "bound_bt = 50\n",
    "ita = 1.05\n",
    "sample_percentage = 0.4\n",
    "sampler = 'APPrior'\n",
    "plot_truer_estir_topicwise(data_name, topic_ids, target_recall, beta, rho, target_rel_num, sub_percentage, bound_bt, ita, sample_percentage, sampler)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_name = 'clef2017'\n",
    "topic_ids = clef_2017_test_topics \n",
    "target_recall = 0.9\n",
    "beta = 1000.0\n",
    "rho = 'dynamic'\n",
    "target_rel_num = 15\n",
    "sub_percentage = 0.8\n",
    "bound_bt = 70\n",
    "ita = 1.05\n",
    "sample_percentage = 0.3\n",
    "sampler = 'APPrior'\n",
    "\n",
    "plot_truer_estir_topicwise(data_name, topic_ids, target_recall, beta, rho, target_rel_num, sub_percentage, bound_bt, ita, sample_percentage, sampler)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_name = 'clef2017'\n",
    "topic_ids = clef_2017_test_topics \n",
    "target_recall = 1.0\n",
    "beta = 1000.0\n",
    "rho = 'dynamic'\n",
    "target_rel_num = 15\n",
    "sub_percentage = 1.0\n",
    "bound_bt = 110\n",
    "ita = 1.05\n",
    "sample_percentage = 0.2\n",
    "sampler = 'APPrior'\n",
    "plot_truer_estir_topicwise(data_name, topic_ids, target_recall, beta, rho, target_rel_num, sub_percentage, bound_bt, ita, sample_percentage, sampler)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AP/Recall/RE as a function of cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def plot_recall_cost_curve(data_name, topic_ids):\n",
    "\n",
    "    autostop_df = load_data(data_name=data_name, folders=['autostop_cttopicwise_md2_c1.0_sp1.0_sr1.0_smpAPPriorSampler_tr1.0_scstrict1'], exp_ids=['1'], topic_ids=topic_ids, func=InteractionResultReader)\n",
    "    autotar_df = load_data(data_name=data_name, folders=['autotar_sp1.0_sr1.0_cttopicwise_md2_c1.0'], exp_ids=['1'], topic_ids=topic_ids, func=InteractionResultReader)\n",
    "    \n",
    "    params = {'xtick.labelsize': 12,\n",
    "             'ytick.labelsize': 12}\n",
    "    matplotlib.rcParams.update(params)\n",
    "    matplotlib.rcParams[\"legend.loc\"]='lower right'\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(8.27*0.6, 8.27*0.6)) \n",
    "    ax2 = ax.twinx() \n",
    "    sns_plot=sns.lineplot(x='percentage', y='recall', hue='model_name', style='model_name', ax=ax, markers=['o'], data=autostop_df)\n",
    "    \n",
    "    sns_plot=sns.lineplot(x='percentage', y='relative_error', hue='model_name', style='model_name', ax=ax2, markers=['<'], data=autostop_df)\n",
    "    \n",
    "    sns_plot=sns.lineplot(x='percentage', y='recall', color='grey', ax=ax, data=autotar_df)\n",
    "    \n",
    "    ax.set_xlabel('$Cost$')\n",
    "    ax.set_ylabel('$Recall$', rotation=90)\n",
    "    ax2.set_ylabel('$RE$', rotation=90)\n",
    "    ax.set_ylim(0, 1.05) \n",
    "    ax2.set_ylim(0, 1.05) \n",
    "    ax2.grid(False)\n",
    "    ax.get_legend().set_visible(False)\n",
    "    ax2.get_legend().set_visible(False)\n",
    "    figure = sns_plot.get_figure() \n",
    "\n",
    "    figure.savefig(os.path.join(paperfiguredir, '{}-recall-re.pdf'.format(data_name)))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_name = 'clef2017'\n",
    "topic_ids = clef_2017_test_topics \n",
    "\n",
    "plot_recall_cost_curve(data_name, topic_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "autotar_df_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.lines as mlines\n",
    "params = {'legend.fontsize': 10,\n",
    "          'legend.handlelength': 1}\n",
    "matplotlib.rcParams.update(params)\n",
    "matplotlib.rcParams[\"legend.loc\"]='upper left'\n",
    "\n",
    "colors = sns.color_palette(\"muted\", 2)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8.27*0.5, 8.27*0.5)) \n",
    "# ax2 = ax.twinx() \n",
    "\n",
    "ax = sns.lineplot(x='percentage', y='recall', color=colors[0], marker='o', ax=ax, legend=False, data=autostop_df_mean)\n",
    "# ax2 = sns.lineplot(x='percentage', y='relative_error', color=colors[1], marker='D', ax=ax2, legend=False, data=autostop_df_mean)\n",
    "ax = sns.lineplot(x='percentage', y='recall',  color='black', marker='o', ax=ax, legend=False, data=autotar_df_mean)\n",
    "\n",
    "ax.set(xlabel='$cost$', ylabel='$recall$', xlim=[0,1], ylim=[0,1.1])\n",
    "# ax2.set(xlabel='$cost$', ylabel='$RE$', xlim=[0,1], ylim=[0,1.1])\n",
    "\n",
    "# handle1 = mlines.Line2D([], [],  marker='o', markersize=6, color=colors[0])\n",
    "# handle2 = mlines.Line2D([], [],  marker='D', markersize=6, color=colors[1])\n",
    "# ax.legend([handle1, handle2], ['$recall_c$', '$RE$'])\n",
    "# ax.annotate('AutoTAR', color='black', fontsize=12, xy=(0.4, 1.0), xytext=(0.4, 1.05),\n",
    "#                              arrowprops={\"arrowstyle\":\"simple\", \"color\":'black'})\n",
    "\n",
    "fig.savefig(os.path.join(paperfiguredir, '{}-{}-{}-recall-re.pdf'.format(data_name, estimator, sampler)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.lines as mlines\n",
    "params = {'legend.fontsize': 10,\n",
    "          'legend.handlelength': 1}\n",
    "matplotlib.rcParams.update(params)\n",
    "matplotlib.rcParams[\"legend.loc\"]='upper left'\n",
    "\n",
    "colors = sns.color_palette(\"muted\", 2)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8.27*0.5, 8.27*0.5)) \n",
    "# ax2 = ax.twinx() \n",
    "\n",
    "ax = sns.lineplot(x='percentage', y='ap', color=colors[0], marker='o', ax=ax, legend=False, data=autostop_df_mean)\n",
    "# ax2 = sns.lineplot(x='percentage', y='relative_error', color=colors[1], marker='D', ax=ax2, legend=False, data=autostop_df_mean)\n",
    "ax = sns.lineplot(x='percentage', y='ap',  color='black', marker='o', ax=ax, legend=False, data=autotar_df_mean)\n",
    "\n",
    "ax.set(xlabel='$cost$', ylabel='$AP$', xlim=[0,1], ylim=[0,1.1])\n",
    "# ax2.set(xlabel='$cost$', ylabel='$RE$', xlim=[0,1], ylim=[0,1.1])\n",
    "\n",
    "# handle1 = mlines.Line2D([], [],  marker='o', markersize=6, color=colors[0])\n",
    "# handle2 = mlines.Line2D([], [],  marker='D', markersize=6, color=colors[1])\n",
    "# ax.legend([handle1, handle2], ['$AP$', '$RE$'])\n",
    "# ax.annotate('AutoTAR', color='black', fontsize=12, xy=(0.4, 0.74), xytext=(0.3, 0.9),\n",
    "#                              arrowprops={\"arrowstyle\":\"simple\", \"color\":'black'})\n",
    "\n",
    "fig.savefig(os.path.join(paperfiguredir, '{}-{}-{}-ap-re.pdf'.format(data_name, estimator, sampler)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
