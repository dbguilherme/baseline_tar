{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes\n",
    "We reproduce the SCAL approach in [1]. We compare our results with that in [1].\n",
    "\n",
    "[1] Gordon V. Cormack and Maura R. Grossman. 2015. Autonomy and Reliability of Continuous Active Learning for\n",
    "Technology-Assisted Review. CoRR abs/1504.06868 (2015). arXiv:1504.06868 http://arxiv.org/abs/1504.06868\n",
    "\n",
    "\n",
    "[2] Evangelos Kanoulas, Dan Li, Leif Azzopardi, and Rene Spijker. 2017. CLEF 2017 technologically assisted reviews in\n",
    "empirical medicine overview. In CEUR Workshop Proceedings, Vol. 1866. 1–29."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import glob\n",
    "import subprocess\n",
    "import pandas as pd\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = os.path.abspath('../..')\n",
    "tar_master_dir = os.path.join(\"root_dir\", 'tar-master')\n",
    "ret_dir = os.path.join(root_dir, 'baseline_reproduction', 'autotar-knee', 'ret', 'tar_run')\n",
    "\n",
    "clef2017_test_30_topics = [\n",
    "'CD007431',\n",
    "'CD008803',\n",
    "'CD008782',\n",
    "'CD009647',\n",
    "'CD009135',\n",
    "'CD008760',\n",
    "'CD010775',\n",
    "'CD009519',\n",
    "'CD009372',\n",
    "'CD010276',\n",
    "'CD009551',\n",
    "'CD012019',\n",
    "'CD008081',\n",
    "'CD009185',\n",
    "'CD010339',\n",
    "'CD010653',\n",
    "'CD010542',\n",
    "'CD010896',\n",
    "'CD010023',\n",
    "'CD010772',\n",
    "'CD011145',\n",
    "'CD010705',\n",
    "'CD010633',\n",
    "'CD010173',\n",
    "'CD009786',\n",
    "'CD010386',\n",
    "'CD010783',\n",
    "'CD010860',\n",
    "'CD009579',\n",
    "'CD009925'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "- The dataset is the test set of the CLEF TAR 2017 dataset. \n",
    "- The dataset is downloaded from the github repository of [**tar-master**](https://github.com/CLEF-TAR/tar).\n",
    "- We make sure the downloaded data is the consistent with what reported in [2]. We compare our result with that Table 1 in [2]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'defaultdict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-01d5ca2848f5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mabs_qrel_dct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_qrel_label\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtar_master_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'2017-TAR/testing/qrels/qrel_abs_test.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mcontent_qrel_dct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_qrel_label\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtar_master_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'2017-TAR/testing/qrels/qrel_content_test.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{:10}\\t{:10}\\t{:10}\\t{:10}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'topic_id'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'qid'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'#abs_qrel'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'#content_qrel'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-01d5ca2848f5>\u001b[0m in \u001b[0;36mload_qrel_label\u001b[0;34m(filepath)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_qrel_label\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mdct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'defaultdict' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "def load_qrel_label(filepath):\n",
    "    dct = defaultdict(set)\n",
    "    with open(filepath) as f:\n",
    "        for line in f:\n",
    "            # CD007431     0  7072537      0  \n",
    "            qid, dummy, pid, label = line.split()\n",
    "            dct[qid].add((pid, label))\n",
    "    return dct\n",
    "\n",
    "\n",
    "abs_qrel_dct = load_qrel_label(os.path.join(\"\", '/home/guilherme/Downloads/temp/tar/2017-TAR/testing/qrels/qrel_abs_test.txt'))\n",
    "content_qrel_dct = load_qrel_label(os.path.join(\"\", '/home/guilherme/Downloads/temp/tar/2017-TAR/testing/qrels/qrel_content_test.txt'))\n",
    "print('{:10}\\t{:10}\\t{:10}\\t{:10}'.format('topic_id', 'qid', '#abs_qrel', '#content_qrel'))\n",
    "for qid in clef2017_test_30_topics:\n",
    "    print('{:10}\\t{:10}\\t{:10}\\t{:10}'.format(qid, len(abs_qrel_dct[qid]),\n",
    "                                       len([pid for pid, label in abs_qrel_dct[qid] if label == '1']),\n",
    "         len([pid for pid, label in content_qrel_dct[qid] if label == '1'])\n",
    "         ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename='./images/table1.png', width=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results in [2]\n",
    "- The results of Autotar and Knee are generated from the Waterloo runs in the CLEF TAR 2017 track.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename='./images/waterloo_table3.png', width=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename='./images/waterloo_table2.png', width=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reproduce AutoTAR  and  Knee \n",
    "- We reproduce the **method A version** (which uses abstract-level relevance labels) described in the paper [2].\n",
    "    \n",
    "- We sweep hyper-parameters in order to reproduce the autotar and knee methods as close as possible.\n",
    "    - **corpus**: the corpus used to construct word-representations for each document, it can be either the documents of the complete 30 topics or only the current topic.\n",
    "    - **min_df**: minimum word frequency, it can be either 2, 3, or 5.\n",
    "    - **C**: the weight of regularization for the logistic regression model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_all_run(model_type='autotar'):\n",
    "    for root, dirs, files in list(os.walk(ret_dir)):\n",
    "        if model_type not in root:\n",
    "            continue\n",
    "\n",
    "        runfile = root+'/all.run'\n",
    "\n",
    "        if len([file for file in files if 'CD' in file]) == 30:\n",
    "            # creat all.run\n",
    "#             print('Creating', runfile)\n",
    "            try:\n",
    "                os.remove(runfile)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            with open(runfile, 'w') as f:\n",
    "                for file in files:\n",
    "                    with open(root+'/'+file) as fr:\n",
    "                        f.write(fr.read())\n",
    "                        \n",
    "    return\n",
    "\n",
    "def load_all_run(model_type='autotar'):\n",
    "    data = []\n",
    "\n",
    "    for root, dirs, files in list(os.walk(ret_dir)):\n",
    "        if model_type not in root:\n",
    "            continue\n",
    "\n",
    "        if len([file for file in files if 'CD' in file]) == 30:\n",
    "\n",
    "            if 'all.run' not in files:\n",
    "                print('all.run does not exists.')\n",
    "\n",
    "            # tar_eval\n",
    "            script = os.path.join(tar_master_dir, 'scripts/tar_eval.py')\n",
    "            qrelfile = os.path.join(tar_master_dir, '2017-TAR/testing/qrels/qrel_content_test.txt')\n",
    "            runfile = root+'/all.run'\n",
    "            ret = subprocess.check_output(['python', script, qrelfile, runfile])\n",
    "\n",
    "\n",
    "            # parse result\n",
    "            ret = subprocess.check_output([' tail -27 '], shell=True, input=ret)\n",
    "            ret = ret.decode(encoding='utf-8')\n",
    "\n",
    "            dct = {}\n",
    "            for line in ret.split('\\n'):\n",
    "\n",
    "                if line != '':\n",
    "                    topic_id, key, val = line.split()\n",
    "                    dct[key] = float(val)\n",
    "\n",
    "\n",
    "            folder = root[:-2]\n",
    "\n",
    "            expid = os.path.basename(root)\n",
    "            ct = re.findall('_ct.*_', folder)[0]\n",
    "            md = re.findall('md.*_', folder)[0]\n",
    "            c = re.findall('c\\d+\\.{0,1}\\d+', folder)[0]\n",
    "\n",
    "            lst = [expid, ct, md, c]\n",
    "            for key in dct:\n",
    "                lst.append(dct[key])\n",
    "\n",
    "            data.append(lst)\n",
    "\n",
    "    head = ['expid', 'ct', 'md', 'c' ] + list(dct.keys())\n",
    "    \n",
    "    df = pd.DataFrame(data=data, columns=head)\n",
    "    \n",
    "    df = df.groupby(['ct', 'md', 'c']).mean()\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result of AutoTAR\n",
    "- The result is close to Table 3. \n",
    "- We report metrics that are also reported in [2], they are ap – average precision, norm_area – area under the cumulative recall curve normalized by the optimal area, and last_rel – minimum number of documents returned to retrieve all relevant documents. With the best configuration we achieve an ap score of 0.191, an norm_area score of 0.947, and an last_rel score of 493, and the metrics are 0.189, 0.948 and 461 reported in [2]. We use the same configuration for all the latter methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_all_run('autotar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['ap', 'norm_area','last_rel']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result of Knee\n",
    "- The result is close to Table 2.\n",
    "- Similar to AutoTAR, we also swept over all possible configurations of the ranking model. The best configuration is the same with that of AutoTAR. With the best configuration (`C = 1.0, min_df = 2, corpus = topicwise`) we achieve an loss_er score of 0.610 and an recall score of 0.999, and the metrics are 0.657, 1.000, reported in [2].\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_all_run('knee')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['num_rels', 'rels_found','r', 'num_docs', 'num_shown', 'loss_r', 'loss_e', 'loss_er']]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
